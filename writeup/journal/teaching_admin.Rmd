---
title: "Modeling optimal classroom teaching as communication with variable listeners"

# Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
author:
  - name: Larry Liu
    affiliation: a
  - name: Michael C. Frank
    affiliation: a,1

address:
  - code: a
    address: Department of Psychology, Stanford University, Stanford, CA 94305

corresponding_author:
  - code: 1
    text: "To whom correspondence should be addressed. Email: mcfrank@stanford.edu"

lead_author_surname: Liu

author_contributions: |
  Author contributions: LL and MCF designed models, wrote code, analyed data, and wrote the paper.

conflict_of_interest: |
  The authors declare no conflict of interest.

abstract: | 
   A good teacher is a good communicator. But communicating to a large audience can be difficult if audience members have different preconceptions, leading them to construe the same message differently. Following this analogy, we develop a framework for modeling classroom education as optimal communication to a variable audience. We study teaching games where teachers provide examples of a target concept to groups of students. Students synthesize these examples with their prior knowledge in order to induce the concept. In this framework, we consider strategies for managing variability, including ability grouping ("tracking"), reductions in class size, and formative assessment. With known costs on actions, our model can also be extended for decision-theoretic analysis.

significance: | 
  Given limitations on time and money, should teachers and school administrators invest in reducing class sizes, should they track students by ability, or should they focus on restructuring instruction in individual classes? Much educational research focuses on what practical gains are achievable due to interventions. This work is valuable, but often proceeds without a theoretical prediction as to the maximal achievable gain. We develop a framework for predicting the performance of optimal teachers in simple teaching scenarios where all constraints can easily be quantified. This framework allows for prediction of the effects of different interventions, for example highlighting the limited effects that should be expected due to many class size interventions. 

acknowledgements: | 
  Thanks to NSF BCS \#1456077 for support, and to Noah Goodman, Long Ouyang, and Michael Henry Tessler for helpful feedback.

keywords: 
  - classroom teaching
  - communication
  - bayesian modeling
  - class size
  - formative assessment

pnas_type: pnasresearcharticle

bibliography: teaching.bib 

csl: pnas.csl

output: rticles::pnas_article 
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      cache = TRUE, sanitize = TRUE,
                      out.width = "3.25in", out.height = "2.6in", 
                      fig.width = 5, fig.height=4, fig.crop = FALSE, 
                      fig.pos = "t", fig.path = 'figs/')
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(directlabels)
library(forcats)
library(stringr)
library(langcog) #requires devtools::install_github("langcog/langcog")
```

A good teacher is a good communicator, choosing explanations and examples that allow students to learn concepts effectively. But just as a message that is intended for one particular person can be incomprehensible to another, a lesson that is appropriate for one student can be inappropriate for another. The perfect pace for one student is too fast or too slow for the second; a helpful analogy for one is confusing to another other. More generally, the broader the audience for a lesson, the more difficult it can be to ensure that every student learns. In this paper, we explore this intuition formally, creating a framework for understanding the role of student variability in educational outcomes. 

Managing student variability is a central issue in educational policy and practice. Should schools group students into classrooms by ability ["tracking"; @glass1979;@slavin1989]? Should policymakers invest resources in decreasing class size, allowing for smaller classes with more opportunities to tailor instruction to individual students [@slavin1987,@tomlinson1999]? Should teachers conduct formative assessments to learn more about the range of knowledge and abilities in a classroom [@fuchs1986;@sadler1989]? Each of these questions has been the focus of intense debate. Randomized trials can provide real-world evidence for the effects of particular interventions on class size or Ã¥bility grouping [e.g., @duflo2011, @duflo2015], but such experiments can tell us only what *does* work, not what could or could not work, in principle. Yet, to our knowledge, no body of theory provides guidance on this issue. Our goal here is to take a first step towards such a framework.

We exploit the parallel between teaching and communication, which emerges in recent formal work using probabilistic models [@goodman2016]. For example, @shafto2014 model the process of reasoning about which examples the teacher should choose and what inference the student should draw from the examples. The unit of their analysis is a "teaching game" [@shafto2014,@avrahami1997], by analogy to "signaling games" in linguistics [@lewis2008]. The goal of creating this abstraction is to create a minimal unit of educational practice that can be analyzed to provide insight into more complex, messier situations. 

In a teaching game, a teacher attempts to teach a target concept to a group of one or more students, by providing examples of that concept. Each student then attempts to induce the target concept based on their prior knowledge and the given examples. Teaching games are collaborative games: the payoff is successful commmunication such that the student's learned concept is closer to the target. We formalize this payoff through the information-theoretic notion of information gain: the distance that the student's concept moves in the direction of the teacher's. 

To explore limits on teaching imposed by structural features of the teaching game, we examine outcomes when both teachers and learners perform optimally. Students in our models are optimal learners, who use Bayesian inference to combine observed examples of a concept with their prior beliefs. Teachers are optimal communicators, who consider their students' prior distribution and choose the best examples to maximize learning. Even under these idealizing assumptions, student learning is necessarily limited. Our analysis investigates sources of these limitations: in particular, the choice of instructional groups, strategies, and teaching examples given uncertainty about student knowledge and abilities. 

Uncertainty about student knowledge and abilities plays an important role in our framework. If teachers know exactly what students know, they can choose appropriate examples. But if they are uncertain, they can guess by integrating their evidence about student abilities; such guesses will be less accurate, leading to lower information gain. Another alternative is to conduct formative assessments to reduce uncertainty about students. Such assessments take time away from instruction, but may nonetheless lead to increased learning due to the information they provide to teachers. We investigate this tradeoff. 

In the remainer of the paper, we first present our teaching game and modeling framework and then describe results that emerge from this framework through simulation. We describe simulations focused on the three topics introduced above: ability grouping, class size, and formative assessment. We end by discussing how our framework could be used to do "optimal administration": given a known cost structure, resources can be allocated to maximize information gain under a variety of different objective functions. 

# Model

In a teaching game, teacher $T$ conveys a target concept $C$ to students $S = {s_1 ... s_n}$. The teacher conveys information by choosing examples $E = {e_1 ... e_m}$ based on some estimate of the students' prior knowledge and abilities $\hat{S} = {\hat{s_1} ... \hat{s_m}}$. Learners in turn attempt to recover $C$ with maximal fidelity. 

Our specific teaching game is to learn the weight on a biased coin (a single Bernoulli variable). Students are modeled as Bayesian (optimal) estimators of the target Bernoulli parameter, using a Beta-Bernoulli distribution. This conjugate model is very convenient: The form of the prior distribution is $Beta(\alpha,\beta)$, and the form of the posterior can be written $Beta(\alpha+x,\beta+y)$ where $x$ and $y$ represent the number of heads (1s) and tails (0s) observed in the data.



<!-- Under this formulation, the prior controls both the speed at which a student will learn and their overall bias. For example, as $\alpha$ and $\beta$ both go towards 0, the student's estimate converges to a maximum-likelihood estimate based on the observed data alone. In contrast, as $\alpha$ and $\beta$ both get larger, the student makes less and less use of the data and is more and more reliant on the shape of the prior distribution. The relative weights of $\alpha$ and $\beta$ control the student's mean estimate---greater pseudo-counts on one or the other will lead to greater bias to believe that the correct parameter is lower or higher.  -->


Payoffs are determined via the change in the learners' estimates based on the chosen example. We assess the Kullback-Leibler divergence [@cover2012] between student knowledge ($B_S$) and the teacher's target distribution ($B_T$) both before and after teaching. The difference between these quantities gives the information gain for a single example $e$:

$$IG(E) = \sum_{s \in S}{D_{KL}(T || S) - D_{KL}(T || {S+E})}$$


We derived a closed form expression for information gain, see SI. 

The job of the teacher is then to choose the optimal distribution of examples to maximize student learning:

$$\underset{E}{\operatorname{argmax}}  {IG(E)}$$

\noindent This optimal set can be computed by assuming that the teacher represents each student as a Beta-Bernoulli distribution. In conditions of perfect knowledge, the teacher has access to the students' true distributions. Teachers with uncertain knowledge initially represent students as having weak uniform beliefs (e.g., $\beta(1,1)$). They can update these representations based on the integration of assessments, which are assumed to be samples from student's true distribution. Assessments are integrated by using them as updates to the teacher's internal representation of individual students' distributions. 


Because it is based on learning a single target concept, our model does not distinguish knowledge and ability.


# Simulations

```{r}
d <- read_csv("../../sims_admin/cached_data/raw_data_041017_3Per11Nu_exp_large.csv") %>%
  mutate(grouping = ifelse(str_detect(simType, "unsorted"), "random", "by ability"),
         knowledge = ifelse(str_detect(simType, "Perfect"), "perfect", "uncertain"),
         knowledge = ifelse(str_detect(simType, "Naive"), "naive", knowledge)) %>%
  mutate(knowledge = fct_rev(knowledge))
#df <- read_csv("../sims_admin/cached_data/raw_data_053017_10timesteps_small.csv")
# df2 <- read_csv("../../sims_admin/cached_data/raw_data_050917_highResMus.csv")
# studentByStudent_df <- read_csv("../../sims_admin/cached_data/raw_data_052317_studentTracker.csv")
```

We investigate how different structural features of the teaching game affect maximal information gain through numerical simulation. We do this by generating random "schools" of 100 students, and systematically varying five parameters:

- The number of teachers in the school, from a single class of 100 to 10 classes of 10,
- The number of formative assessments conducted by each teacher out of 12 instructional periods, from 0 (0%) to 12 (100%),
- How extreme the teacher's target concept is, from a concept for which students have very different starting points (.5) to a more extreme concept for which students will have more similar starting points (.9),
- Whether the teachers have perfect or uncertain knowledge about their teachers, and 
- Whether teachers group students into classes randomly or by ability.

\noindent For each parameter setting, we generate 100 schools, examine the effects of these design choices, and then average performance. All simulations were conducted using the probabilistic programming language \texttt{WebPPL} [@goodman2017]. All code 
available at [http://github.com/mcfrank/teaching]().


```{r, fig.cap="\\label{fig:ex} Average information gain plotted by the number of assessments. Colors show whether students are grouped by ability or randomly, and line-type shows teacher knowledge (uncertain, perfect, or naive). The dots show the number of assessments that produces the maximal information gain."}
ig <- d %>%
  filter(exponent == 1, teacherMu %in% .6, numTeachers %in% c(5)) %>%
  group_by(grouping, knowledge, numTeachers, numAssessments, teacherMu) %>%
  summarise(IG = mean(IG))

maxes <- ig %>%
  filter(knowledge == "uncertain") %>%
  group_by(grouping, knowledge, teacherMu, numTeachers) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]],
            IG = max(IG))
  
ggplot(ig, 
       aes(x = numAssessments, y = IG, col = grouping, lty = knowledge)) +
  geom_line() +
  geom_point(data = maxes, size = 4) + 
  # facet_grid(numTeachers~teacherMu) + 
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12)) +
  scale_color_solarized(name="Grouping") + 
  scale_linetype_discrete(name = "Teacher Knowledge") + 
  xlab("Number of Assessments") +
  ylab("Information Gain (nats)") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
  
```

Figure \ref{fig:ex} shows an example parameter regime (target concept = .6, 5 teachers). A number of trends are immediately apparent. First, the overall amount of possible information gain varies as a function of the number of assessments. In the limit (100% assessments), students are never taught anything and they do not learn at all. Second, there is a substantial boost in total information gain due to ability grouping (red vs. blue) for perfect teachers (dotted lines). Third, and perhaps most interesting, this gain is moderated in uncertain teachers, who must conduct assessments to group students successfully. With no assessments, ability grouping is no better than random, but as the number of assessments is increased, the groupings approach perfect (though instructional opportunities as a whole decline). In this parameter regime, the optimal solution is to group by ability and administer three assessments.

In the following simulations, we attempt to quantify the advantages due to different decisions across parameter regimes. 

## Ability grouping and class size

## Perfect ability grouping yields large gains in performance

Generalizing the result in Figure \ref{fig:ex}, we examine the effects of student grouping, in which students are placed in classes based on the teacher's evaluation of their knowledge or abilities. We begin by considering the case of perfect teachers, in order to estimate the maximal possible information gain based on grouping. We compute the proportional information gain due to sorting as a proportion of baseline information gain due to random examples: 

$$\frac{IG_{sorted} - IG_{unsorted}}{IG_{naive}}$$

```{r, fig.cap="\\label{fig:grouping} Proportional information gain due to grouping plotted by class size. Colors show different target concepts. Error bars show 95\\% confidence intervals computed by bootstrap resampling."}

grouping_boost <- d %>%
  filter(exponent == 1, numAssessments == 0, knowledge %in% c("perfect","naive")) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu) %>%
  spread(simType, IG) %>%
  group_by(trialNum, numTeachers, teacherMu) %>%
  mutate(grouping_boost = (sortedPerfectTeachers - unsortedPerfectTeachers)/unsortedNaiveTeachers) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "grouping_boost")
  
ggplot(grouping_boost, 
       aes(x = 100/numTeachers, y = mean, col = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position = position_dodge(width = 2)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  scale_x_reverse(breaks = round(100/c(1, 2, 3, 5, 10))) +
  ylim(-.1,1.5) + 
  scale_color_solarized(name="Target Concept") + 
  xlab("Class Size (Students)") +
  ylab("Proportional IG") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
```

Figure \ref{fig:grouping} shows the results of this analysis. Again, several trends are apparent. First, all trends are positive. As class sizes decrease, information gain increases over baseline, sometimes substantially (proportion increase > 1 indicates a more than doubling of information gain). Second, gains due to ability grouping are most apparent for less extreme target concepts -- that is, the more students differ in their intial knoewledge/abilities with respect to these concepts, the more grouping matters. Third, class size plays a relatively small role. Ability grouping is not possible in our games when there is only one teacher, but beyond two teachers, additional gains are marginal. Thus, within our teaching games, our simulations show that grouping students into two rough "tracks" will produce substantial benefits, especially for concepts where students' starting points are variable. 

### Class size reduction yields small effects


```{r, fig.cap="\\label{fig:class_size} Proportional information gain due reduction in class size. Colors show different target concepts. Error bars show 95\\% confidence intervals computed by bootstrap resampling."}

baseline_boost <- d %>%
  filter(exponent == 1, numAssessments == 0, grouping == "random",
         knowledge %in% c("perfect","naive")) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu) %>%
  spread(simType, IG) %>%
  group_by(teacherMu, trialNum) %>%
  mutate(baseline_boost = (unsortedPerfectTeachers - unsortedPerfectTeachers[numTeachers==1]) / unsortedPerfectTeachers[numTeachers==1]) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "baseline_boost")
  
ggplot(baseline_boost, 
       aes(x = 100/numTeachers, y = mean, col = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  scale_x_reverse(breaks = round(100/c(1, 2, 3, 5, 10))) +
  ylim(-.1,1.5) + 
  scale_color_solarized(name="Target Concept") + 
  xlab("Class Size (Students)") +
  ylab("Proportional IG") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom")
```


Figure \ref{fig:class_size} shows a reanalysis of the same data, but highlighting class size effects (absolute information gain) in the absence of ability grouping. Here the measure is information gain proportional to the case where there is only a single instructor, shown on the same scale as Figure \ref{fig:grouping}. Class size reductions lead to increases in information gain, especially for less extreme concepts, but increases are tiny compared to those due to ability grouping. 

The intuition guiding this difference is relatively straightforward. Variance between students in our model is the primary factor governing information gain, with lower variance leading to greater IG. Thus,  class size effects in this model are due exclusively to chance reductions in variance. On average, the range of student abilities in a class will be somewhat smaller for a smaller class, leading to an increased ability for the teacher to customize to that class. But down to class sizes of only a handful of students, these effects are marginal compared with direct reductions of variance by ability grouping. 

## Formative assessment

In the next set of simulations, we examine the effects of formative assessment in more detail. In these simulations, teachers are uncertain about student knowledge and only gain information by sacrificing class time to conduct assessments. Each assessment provides a small increment of information (since assessment is modeled as random sampling from students' true distributions), but larger amounts of assessment are costly in terms of instruction time. 

### Imperfect ability grouping yields lower effect sizes

```{r, fig.cap="\\label{fig:imperfect_grouping} Proportional information gain due to grouping using imperfect information, for one target concept (.5). Colors show different numbers of assessments. Error bars show 95\\% confidence intervals computed by bootstrap resampling."}

grouping_boost_imperfect <- d %>%
  filter(exponent == 1, knowledge %in% c("uncertain","naive"), 
         teacherMu == .5,
         numAssessments < 10) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu, numAssessments) %>%
  spread(simType, IG) %>%
  group_by(trialNum, numTeachers, teacherMu, numAssessments) %>%
  mutate(grouping_boost = (sortedUncertainTeachers - unsortedUncertainTeachers)/unsortedNaiveTeachers) %>%
  group_by(numTeachers, teacherMu, numAssessments) %>%
  multi_boot_standard(col = "grouping_boost")
  
ggplot(grouping_boost_imperfect, 
       aes(x = 100/numTeachers, y = mean, col = numAssessments/12, group = numAssessments/12)) + 
  geom_line() + 
  # geom_linerange(aes(ymin = ci_lower, ymax = ci_upper),
  #                position = position_dodge(width = 2)) +
  geom_hline(yintercept = 0, lty = 2) + 
  scale_x_reverse(breaks = round(100/c(1, 2, 3, 5, 10))) +
  ylim(-.1,3) +
  # facet_wrap(~teacherMu) + 
  scale_color_gradient(name="Proportion Assessment", breaks = c(0,.25,.5, .75)) + 
  xlab("Class Size (Students)") +
  ylab("Proportional IG") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
```

Although perfect ability grouping yields large increases in information gain, as shown above, these increases are mostly not realized in cases of imperfect knowledge. Figure \ref{fig:imperfect_grouping} shows proportional information gain due to ability grouping for uncertain teachers. Gains grow with additional assessment, but maximal gains are not achieved until nearly all class time is used for assessment. More modest amounts of assessment lead to much more limited proportional gains, however. In addition, by devoting class time to assessment, absolute information gain may decrease.

### A small amount of assessment produces maximal absolute gains

How much assessment produces the best possible information gain, optimizing the tradeoff between assessment and instruction? Figure \ref{fig:temporal_tradeoff} shows the number of assessments that yielded the maximal information gain for each target concept, for both ability-grouped and random class assignemnts. In both cases, the optimal number of assessments is relatively limited, especially for more extreme concepts. For concepts with more student variability (e.g., $\mu = .5$), performance in random classes increases somewhat. This increase is steeper for classes with ability grouping, supporting the result described above: to see maximal performance in ability grouping, teachers must know a large amount about each individual student's abilities.   

```{r, fig.cap="\\label{fig:temporal_tradeoff} Absolute information gain plotted by the optimal number of assessments, for students either grouped by ability or randomly assigned to classes. Colors show different target concepts, and dashed lines show the best fitting quadratic.}

ig_maxes <- d %>%
  filter(exponent == 1, knowledge == "uncertain") %>%
  group_by(teacherMu, grouping, numAssessments) %>%
  summarise(IG = mean(IG)) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]],
            IG = max(IG))

ggplot(ig_maxes, 
       aes(x = numAssessments, y = IG)) + 
  # geom_jitter( width = .2, height = .2)+ 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se=FALSE, col = "black", lty = 2) +
  geom_point(aes(col = factor(teacherMu)), size = 4) + 
  facet_grid(. ~ grouping) + 
  ggthemes::scale_color_solarized(name="Target Concept") + 
  xlab("Optimal number of assessments") +
  ylab("Information Gain (nats)") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
```


## Decision-Theoretic Applications

<!-- - when assessment dominates, get lots of teachers. (uninteresting) -->
<!-- - when teaching cost dominates,  -->
<!--   - perfect teachers have a pareto frontier where it's still worth assessing sometimes -->
<!--   - but uncertain teachers don't usually - seems there you just hire one. -->
<!-- - tradeoff - you see frontiers in both -->

In our final set of simulations, we consider an additional factor in making decisions: the costs and payoffs of different interventions. 

We consider how a range of different cost structures can be assigned to decisions, and consider different metrics for choosing information gain. 

We also consider payoff functions that value different students' learning differently, including a "remedial" payoff that incentivizes a minimal level of 




## Pareto frontier for assessments 

Interventions to change class size are costly: they require resources to be allocated to hire more teachers. By assigning a notional "cost" (in arbitrary units) to teachers, it is possible to define a pareto frontier between assessments and teachers, giving the . 

```{r}
ig_by_cost <- d %>%
  mutate(teacherMu = factor(teacherMu)) %>%
  group_by(knowledge, grouping, teacherMu, numTeachers, numAssessments) %>%
  summarise(IG = mean(IG)) %>%
  group_by(knowledge, grouping, teacherMu, numTeachers) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]][1],
            IG = max(IG)[1])
  

ggplot(ig_by_cost,
       aes(x = numTeachers, y = numAssessments, col = teacherMu)) + 
  # geom_text(aes(label = numAssessments), nudge_y = .6) + 
  geom_line() + 
  facet_wrap(~knowledge*grouping)+
  scale_colour_solarized(name = "Target Concept") + 
  scale_x_continuous(breaks = c(1,2,3,5,10)) + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") + 
  xlab("Number of Teachers") + 
  ylab("Number of Assessments")
```

```{r, fig.cap="Optimal"}
optimal_ig_by_cost <- d %>%
  filter(exponent == 1) %>%
  mutate(cost = numAssessments * 50 + numTeachers * (10 + numExamples * 2)) %>%
  group_by(numTeachers, numAssessments, teacherMu, simType, cost) %>%
  summarise(IG = mean(IG)) %>%
  filter(simType == "sortedUncertainTeachers") %>%
  arrange(cost) %>%
  group_by(teacherMu) %>%
  filter(IG == cummax(IG))

ggplot(filter(optimal_ig_by_cost, teacherMu < 0.8),
       aes(x = cost, y = IG)) + 
  geom_text(aes(label = numAssessments, col = factor(numTeachers))) + 
  geom_line() + 
  facet_grid(.~teacherMu)+
  scale_colour_solarized(name = "Number of Teachers") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom",plot.title = element_text(hjust = 0.5)) + 
  xlab("Cost") + 
  ylab("Information Gain (nats)") + 
  ggtitle("IG under Optimal Spending Policy per Cost\nat 0.5 to 0.7 Target Knowledge Levels")

```


```{r, fig.cap="Pareto frontiers"}
ig_by_cost <- d %>%
  filter(exponent == 1) %>%
  mutate(cost = numAssessments * 50 + numTeachers * (10 + numExamples * 2)) %>%
  group_by(numTeachers, numAssessments, teacherMu, simType, cost) %>%
  summarise(IG = mean(IG))

dummy <- data.frame(teacherMu = c(0.5, 0.6, 0.7), Z = c(400, NA, NA))

ggplot(filter(ig_by_cost, teacherMu < 0.8,
              simType %in% c("sortedUncertainTeachers")),
       aes(x = cost, y = IG, col = factor(numTeachers))) + 
  geom_text(aes(label = numAssessments)) + 
  geom_line() + 
  facet_grid(.~factor(teacherMu))+
  scale_colour_solarized(name = "Number of Teachers") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom",plot.title = element_text(hjust = 0.5)) + 
  xlab("Cost") + 
  ylab("Information Gain (nats)") + 
  ggtitle("Cost vs. Information Gain at 0.5 to 0.7 Target Knowledge Levels") +
  geom_vline(data=dummy, aes(xintercept = Z), linetype="dashed")

ig_by_cost <- df %>%
  filter(exponent == 1) %>%
  mutate(cost = numAssessments * 50 + numTeachers * (10 + numExamples * 10)) %>%
  group_by(numTeachers, numAssessments, teacherMu, simType, cost) %>%
  summarise(IG = mean(IG))

ggplot(filter(ig_by_cost, teacherMu < 0.8,
              simType %in% c("sortedUncertainTeachers")),
       aes(x = cost, y = IG, col = factor(numTeachers))) +
  geom_text(aes(label = numAssessments)) +
  geom_line() +
  facet_grid(.~teacherMu)+
  scale_colour_solarized(name = "Number of Teachers") +
  ggthemes::theme_few() +
  theme(legend.position = "bottom") +
  xlab("Cost") +
  ylab("Information Gain (nats)")
```

### Alternative payoff functions 

In our previous simulations, we have attempted to maximize total information gain. This payoff function assumes that all students' learning is treated equally. But it should be possible to consider alternatives that value student learning differently. We consider an expanded payoff function

$$IG_{total}(E,\lambda) = \sum_{s \in S}{ (IG_s(E) ) ^\lambda}$$


\noindent which produces a range of outcomes For $\lambda > 1$, students with higher information gain (who stand to learn more in a particular scenario) dominate the choice of strategy. Strategies chosen under this payoff are effectively "remedial": students who are far from the target concept can catch up more effectively. In contrast, payoffs with $\lambda < 1$ de-emphasize the gains of students who learn more and prioritize students who learn less (due to greater initial knowledge) more equally. 

```{r}
ig_maxes <- d %>%
  filter(knowledge == "uncertain") %>%
  group_by(exponent, teacherMu, grouping, numAssessments) %>%
  summarise(IG = mean(IG)) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]],
            IG = max(IG))

ggplot(ig_maxes, 
       aes(x = numAssessments, y = IG, col = factor(exponent), group = factor(exponent))) + 
  # geom_smooth(method = "lm", formula = y ~ x + I(x^2), se=FALSE, lty = 2) +
  geom_line() + 
  facet_grid(. ~ grouping) + 
  ggthemes::scale_color_solarized(name=expression(lambda)) + 
  xlab("Optimal number of assessments") +
  ylab("Information Gain (nats)") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
```

# General Discussion

### Class size effects

Very controverisial

@hoxby2000 reports very small effects
@mosteller1995 STARS reports larger effects. 

without coming to definitive judgment on the literature, we can say that expected effect sizes are probably relatively small. 

(discuss populations farther from the mean)

### Limitations

* motivation
* class dynamics
* teaching game we choose
* division of assessment and teaching, when actually teaching is informative


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

