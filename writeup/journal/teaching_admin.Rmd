---
title: "Modeling optimal classroom teaching as communication to variable listeners"

# Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
author:
  - name: Larry Liu
    affiliation: a
  - name: Michael C. Frank
    affiliation: a,1

address:
  - code: a
    address: Department of Psychology, Stanford University, Stanford, CA 94305

corresponding_author:
  - code: 1
    text: "To whom correspondence should be addressed. Email: mcfrank@stanford.edu"

lead_author_surname: Liu

author_contributions: |
  Author contributions: LL and MCF designed models, wrote code, analyed data, and wrote the paper.

conflict_of_interest: |
  The authors declare no conflict of interest.

abstract: | 
   A good teacher is a good communicator. But communicating to a large audience can be difficult if audience members have differing preconceptions and hence construe the same message differently. Following this analogy of teaching as communication, we develop a framework for modeling classroom education as optimal communication to a variable audience. We study simple teaching games where teachers provide examples of a target concept to groups of students. Students synthesize these examples with their prior knowledge in order to induce the concept. We consider strategies for managing variability, including ability grouping ("tracking"), reductions in class size, and formative assessment. With known costs on actions, our model can also be extended for decision-theoretic analysis. This model provides a framework for estimating theoretical limits on the utility of educational interventions.

significance: | 
  Given limitations on time and money, should teachers and school administrators invest in reducing class sizes, should they track students by ability, or should they focus on improving instruction in individual classes? Much educational research focuses on what practical gains are achievable due to interventions. This work is valuable, but often proceeds without a theoretical prediction as to the maximal achievable gain. We develop a framework for predicting the performance of optimal teachers in simple teaching scenarios where all constraints can easily be quantified. This framework allows for prediction of the effects of different interventions, for example highlighting the limited effects that should be expected due to many class size interventions. 

acknowledgements: | 
  Thanks to NSF BCS \#1456077 for support, and to Noah Goodman, Long Ouyang, and Michael Henry Tessler for helpful feedback.

keywords: 
  - classroom teaching
  - communication
  - bayesian modeling
  - class size
  - formative assessment

pnas_type: pnasresearcharticle

bibliography: teaching.bib 

csl: pnas.csl

output: rticles::pnas_article 
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      cache = TRUE, sanitize = TRUE,
                      out.width = "3.25in", out.height = "2.6in", 
                      fig.width = 5, fig.height=4, fig.crop = FALSE, 
                      fig.pos = "t", fig.path = 'figs/')
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(directlabels)
library(forcats)
library(stringr)
library(langcog) #requires devtools::install_github("langcog/langcog")
```

```{r}
# CONSTANTS
EXAMPLE_TEACHERMU <- .6
```

A good teacher is a good communicator, choosing explanations and examples that allow students to learn concepts effectively. But just as a message that is intended for one particular person can be incomprehensible to another, a lesson that is appropriate for one student can be inappropriate for another. The perfect pace for one student is too fast or too slow for the second; a helpful analogy for one is confusing to another other. More generally, the broader the audience for a lesson, the more difficult it can be to ensure that every student learns. In this paper, we explore this intuition formally, creating a framework for understanding the role of student variability in educational outcomes. 

Managing student variability is a central issue in educational policy and practice. Should schools group students into classrooms by ability ["tracking"; @glass1979;@slavin1989]? Should policymakers invest resources in decreasing class size, allowing for smaller classes with more opportunities to tailor instruction to individual students [@slavin1987,@tomlinson1999]? When should teachers conduct formative assessments to learn more about the range of knowledge and abilities in a classroom [@fuchs1986;@sadler1989]? Each of these questions has been the focus of intense discussion. Randomized trials can provide real-world evidence for the effects of particular interventions on class size or Ã¥bility grouping [e.g., @duflo2011, @duflo2015], but such experiments can tell us only what *does* work, not what could or couldn't work in principle. Yet, to our knowledge, no body of theory provides guidance on this issue. Our goal here is to take a first step towards such a framework.

We exploit parallel between teaching and communication that emerge in recent formal work using probabilistic models [@shafto2012,@goodman2016]. For example, @shafto2014 model the process of reasoning about which examples the teacher should choose and what inference the student should draw from the examples. The unit of their analysis is a "teaching game" [@shafto2014,@avrahami1997], by analogy to "signaling games" in linguistics [@lewis2008]: scenarios whose participants, rules, and parameters are known and hence which admit quantitative analysis. These abstractions can in turn be leveraged to  provide insight into more complex, messier situations. 

In a teaching game, a teacher attempts to teach a target concept to a group of one or more students, by providing examples of that concept. Each student then attempts to induce the target concept based on their prior knowledge and the given examples. Teaching games are collaborative games: the payoff is successful commmunication such that the student's learned concept is closer to the target. We formalize this payoff through the information-theoretic notion of information gain, the distance that the student's concept moves in the direction of the teacher's through teaching. 

The game we study is extremely simple: a teacher attempts to communicate the weight on a biased coin via showing a series of individual coin flips (heads or tails). The key choice is what set of flips to show. Given unlimited teaching time, a long sequence of flips could allow the learner to approximate the coin's weight very closely. But with more limited time, the teacher must choose selectively and with respect to the students' prior beliefs. These priors -- which stand in for the combination of knowledge and ability differences that vary across students in real classrooms -- are the key factor in determining teaching effectiveness. If two students have very different priors, they will take away different generalizations from the same sequence of examples. Thus, our analysis focuses on managing student variability by intervening on the teaching game. 

To explore the limits imposed by structural features of the teaching game, we examine outcomes when both teachers and learners perform optimally. Students in our models are optimal Bayesian learners, who make statistical inferences about how to combine observed examples of a concept with their prior beliefs. Teachers are optimal communicators, who consider their students' prior distribution and choose the best examples to maximize learning. Even under these idealizing assumptions, student learning is necessarily limited. Our analysis investigates sources of these limitations: in particular, the choice of instructional groups, strategies, and teaching examples given uncertainty about student knowledge and abilities. 

Uncertainty about student priors plays an important role in our framework. If teachers know exactly what students know, they can choose appropriate examples. But if they are uncertain, they can guess, informed by observed evidence about student abilities. Such guesses will necessarily be less accurate, leading to lower information gain. We assume that teachers can gather further information about student priors by conducting assessments. Such assessments take time away from instruction, but may nonetheless lead to increased learning due to the information they provide to teachers. We investigate this tradeoff. 

We begin by presenting the details of our modeling framework and then describe results that emerge from this framework through simulation. We next describe simulations focused on estimating effects of 1) reducing class size, 2) grouping students into classes by ability, and 3) conducting formative assessments to increase teacher knowledge about students. We end by discussing how our framework can be used for decision-making by assigning costs to individual aspects of an intervention. 

# Model

In a teaching game, teacher $T$ conveys a target concept $C$ to students $S = {s_1 ... s_n}$. The teacher conveys information by choosing examples $E = {e_1 ... e_m}$ based on some estimate of the students' prior knowledge and abilities $\hat{S} = {\hat{s_1} ... \hat{s_m}}$. Learners in turn attempt to recover $C$ with maximal fidelity. 

Our specific teaching game is to learn the weight on a biased coin (a single Bernoulli variable). Students are modeled as Bayesian (optimal) estimators of the target Bernoulli parameter, using a Beta-Bernoulli distribution. This conjugate model is very convenient: The form of the prior distribution is $Beta(\alpha,\beta)$, and the form of the posterior can be written $Beta(\alpha+x,\beta+y)$ where $x$ and $y$ represent the number of heads (1s) and tails (0s) observed in the data.



<!-- Under this formulation, the prior controls both the speed at which a student will learn and their overall bias. For example, as $\alpha$ and $\beta$ both go towards 0, the student's estimate converges to a maximum-likelihood estimate based on the observed data alone. In contrast, as $\alpha$ and $\beta$ both get larger, the student makes less and less use of the data and is more and more reliant on the shape of the prior distribution. The relative weights of $\alpha$ and $\beta$ control the student's mean estimate---greater pseudo-counts on one or the other will lead to greater bias to believe that the correct parameter is lower or higher.  -->


Payoffs are determined via the change in the learners' estimates based on the chosen example. We assess the Kullback-Leibler divergence [@cover2012] between student knowledge ($B_S$) and the teacher's target distribution ($B_T$) both before and after teaching. The difference between these quantities gives the information gain for a single example $e$:

$$IG(E) = \sum_{s \in S}{D_{KL}(T || S) - D_{KL}(T || {S+E})}$$


We derived a closed form expression for information gain, see SI. 

The job of the teacher is then to choose the optimal distribution of examples to maximize student learning:

$$\underset{E}{\operatorname{argmax}}  {IG(E)}$$

\noindent This optimal set can be computed by assuming that the teacher represents each student as a Beta-Bernoulli distribution. In conditions of perfect knowledge, the teacher has access to the students' true distributions. Teachers with uncertain knowledge initially represent students as having weak uniform beliefs (e.g., $\beta(1,1)$). They can update these representations based on the integration of assessments, which are assumed to be samples from student's true distribution. Assessments are integrated by using them as updates to the teacher's internal representation of individual students' distributions. 


Because it is based on learning a single target concept, our model does not distinguish knowledge and ability.


# Simulations

```{r}
d <- read_csv("../../sims_admin/cached_data/raw_data_041017_3Per11Nu_exp_large.csv") %>%
  mutate(grouping = ifelse(str_detect(simType, "unsorted"), "random", "by ability"),
         knowledge = ifelse(str_detect(simType, "Perfect"), "perfect", "uncertain"),
         knowledge = ifelse(str_detect(simType, "Naive"), "naive", knowledge)) %>%
  mutate(knowledge = fct_rev(knowledge))
#df <- read_csv("../sims_admin/cached_data/raw_data_053017_10timesteps_small.csv")
# df2 <- read_csv("../../sims_admin/cached_data/raw_data_050917_highResMus.csv")
# studentByStudent_df <- read_csv("../../sims_admin/cached_data/raw_data_052317_studentTracker.csv")
```

We investigate how different structural features of the teaching game affect maximal information gain through numerical simulation. We do this by generating random "schools" of 100 students, and systematically varying five parameters:

- The number of teachers in the school, from a single class of 100 to 10 classes of 10,
- The number of formative assessments conducted by each teacher out of 12 instructional periods, from 0 (0%) to 12 (100%),
- How extreme the teacher's target concept is, from a concept for which students have very different starting points (.5) to a more extreme concept for which students will have more similar starting points (.9),
- Whether the teachers have perfect or uncertain knowledge about their teachers, and 
- Whether teachers group students into classes randomly or by ability.

\noindent For each parameter setting, we generate 100 schools, examine the effects of these design choices, and then average performance. All simulations were conducted using the probabilistic programming language \texttt{WebPPL} [@goodman2017]. All code 
available at [http://github.com/mcfrank/teaching]().


```{r, fig.cap="\\label{fig:ex} Average information gain plotted by the number of assessments. Colors show whether students are grouped by ability or randomly, and line-type shows teacher knowledge (uncertain, perfect, or naive). The dots show the number of assessments that produces the maximal information gain."}
ig <- d %>%
  filter(exponent == 1, teacherMu %in% EXAMPLE_TEACHERMU, numTeachers %in% c(5)) %>%
  group_by(grouping, knowledge, numTeachers, numAssessments, teacherMu) %>%
  summarise(IG = mean(IG))

maxes <- ig %>%
  filter(knowledge == "uncertain") %>%
  group_by(grouping, knowledge, teacherMu, numTeachers) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]],
            IG = max(IG))
  
ggplot(ig, 
       aes(x = numAssessments, y = IG, col = grouping, lty = knowledge)) +
  geom_line() +
  geom_point(data = maxes, size = 4) + 
  # facet_grid(numTeachers~teacherMu) + 
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12)) +
  scale_color_solarized(name="Grouping") + 
  scale_linetype_discrete(name = "Teacher Knowledge") + 
  xlab("Number of Assessments") +
  ylab("Information Gain (nats)") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom", legend.box = "vertical") 
  
```

Figure \ref{fig:ex} shows an example parameter regime (target concept = .6, 5 teachers). A number of trends are immediately apparent. First, the overall amount of possible information gain varies as a function of the number of assessments. In the limit (100% assessments), students are never taught anything and they do not learn at all. Second, there is a substantial boost in total information gain due to ability grouping (red vs. blue) for perfect teachers (dotted lines). Third, and perhaps most interesting, this gain is moderated in uncertain teachers, who must conduct assessments to group students successfully. With no assessments, ability grouping is no better than random, but as the number of assessments is increased, the groupings approach perfect (though instructional opportunities as a whole decline). In this parameter regime, the optimal solution is to group by ability and administer three assessments.

In the following simulations, we attempt to quantify the advantages due to different decisions across parameter regimes. 

### Class size reduction yields small effects

```{r, fig.cap="\\label{fig:grouping} Proportional information gain due reduction in class size, without (left) and with ability grouping (right). Colors show different target concepts. Error bars show 95\\% confidence intervals computed by bootstrap resampling."}

baseline_boost <- d %>%
  filter(exponent == 1, numAssessments == 0, grouping == "random",
         knowledge %in% c("perfect","naive")) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu) %>%
  spread(simType, IG) %>%
  group_by(teacherMu, trialNum) %>%
  mutate(baseline_boost = (unsortedPerfectTeachers - unsortedPerfectTeachers[numTeachers==1]) / unsortedPerfectTeachers[numTeachers==1]) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "baseline_boost") %>%
  mutate(condition = "Class Size Reduction Alone")

grouping_boost <- d %>%
  filter(exponent == 1, numAssessments == 0, knowledge %in% c("perfect","naive")) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu) %>%
  spread(simType, IG) %>%
  group_by(trialNum, numTeachers, teacherMu) %>%
  mutate(grouping_boost = (sortedPerfectTeachers - unsortedPerfectTeachers)/unsortedPerfectTeachers) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "grouping_boost") %>%
  mutate(condition = "+ Ability Grouping")
    
size_grouping <- bind_rows(baseline_boost, grouping_boost) %>%
  mutate(condition = fct_relevel(condition, "Class Size Reduction Alone"))
  
ggplot(size_grouping,  
       aes(x = 100/numTeachers, y = mean, col = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  # geom_hline(yintercept = 0, lty = 2) + 
  scale_x_reverse(breaks = round(100/c(1, 2, 3, 5, 10))) +
  # ylim(-.1,1.5) + 
  facet_grid(.~condition)+ 
  scale_color_solarized(name="Target Concept") + 
  xlab("Class Size (Students)") +
  ylab("Proportional IG") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom")
```

Generalizing the result in Figure \ref{fig:ex}, we examine effects of class size on performnace. We begin by considering the case of perfect teachers, in order to estimate the maximal possible information gain based on reductions in class size. We compute the proportional information gain due to class size reduction, compared to a single large class as baseline:

$$\frac{IG_{N~teachers} - IG_{1~teacher}}{IG_{1~teacher}}$$




\noindent Figure \ref{fig:grouping}, left shows class size effects. Class size reductions lead to increases in information gain, especially for less extreme concepts, but they are small.

The intuition guiding this difference is relatively straightforward. Variance between students in our model is the primary factor governing information gain, with lower variance leading to greater IG. Thus,  class size effects in this model are due exclusively to chance reductions in variance. On average, the range of student abilities in a class will be somewhat smaller for a smaller class, leading to an increased ability for the teacher to customize to that class. But this effect is typically small (a maximum of around 10% in our simulations.

### Perfect ability grouping yields large gains in performance

We next consider the effects of student grouping, in which students are placed in classes based on the teacher's evaluation of their knowledge or abilities. We compute the proportional information gain due to ability grouping similarly, comparing to the ungrouped condition as a baseline. Figure \ref{fig:grouping}, right shows the results of this analysis. Several trends are apparent. First, all trends are positive. As class sizes decrease, information gain increases over baseline, sometimes substantially (up to 80% in some conditions). Second, gains due to ability grouping are most apparent for less extreme target concepts -- that is, the more students differ in their intial knoewledge/abilities with respect to these concepts, the more grouping matters. Third, class size plays a relatively small role: beyond two teachers, additional gains are marginal. Thus, within our teaching games, our simulations show that -- for teachers with perfect knowledge -- grouping students into two rough "tracks" will produce substantial benefits, especially for concepts where students' starting points are variable. 

### Imperfect ability grouping yields lower effect sizes

```{r, fig.cap="\\label{fig:imperfect_grouping} Proportional information gain due to grouping using imperfect information, for one target concept (.6). Colors show different numbers of assessments. Error bars show 95\\% confidence intervals computed by bootstrap resampling."}

grouping_boost_imperfect <- d %>%
  filter(exponent == 1, knowledge %in% c("uncertain","naive"), 
         # teacherMu == EXAMPLE_TEACHERMU,
         numAssessments < 12) %>%
  select(trialNum, simType, IG, numTeachers, teacherMu, numAssessments) %>%
  spread(simType, IG) %>%
  group_by(trialNum, numTeachers, teacherMu, numAssessments) %>%
  mutate(grouping_boost = (sortedUncertainTeachers - unsortedUncertainTeachers)/unsortedUncertainTeachers) %>%
  group_by(numTeachers, teacherMu, numAssessments) %>%
  multi_boot_standard(col = "grouping_boost")
  

ggplot(filter(grouping_boost_imperfect, numTeachers == 2), 
       aes(x = numAssessments/12, y = mean, col = factor(teacherMu), group = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Proportion of Instruction Time Devoted to Assessment") +
  ylab("Proportional IG") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") +
  scale_colour_solarized() + 
  xlim(0,1) 
```

We have previously considered teachers with perfect knowledge of students. But what if teachers are uncertain about student knowledge and only gain information by sacrificing class time to conduct assessments? In this case, each assessment provides a small increment of information (since assessment is modeled as random sampling from students' true distributions), but larger amounts of assessment are costly in terms of instruction time. 
 
Figure \ref{fig:imperfect_grouping} shows proportional information gain due to ability grouping for uncertain teachers. Although perfect ability grouping yields large increases in information gain, these increases are mostly not realized in cases of imperfect knowledge. Gains grow with additional assessment, but maximal gains are not achieved until nearly all class time is used for assessment. More modest amounts of assessment lead to much more limited proportional gains, however. In addition, by devoting class time to assessment, absolute information gain may decrease.

### A small amount of assessment produces maximal absolute gains

How much assessment produces the best possible information gain, optimizing the tradeoff between assessment and instruction? Figure \ref{fig:temporal_tradeoff} shows the number of assessments that yielded the maximal information gain for each target concept, for both ability-grouped and random class assignemnts. In both cases, the optimal number of assessments is relatively limited, especially for more extreme concepts. For concepts with more student variability (e.g., $\mu = .5$), performance in random classes increases somewhat. This increase is steeper for classes with ability grouping, supporting the result described above: to see maximal performance in ability grouping, teachers must know a large amount about each individual student's abilities.   

```{r, fig.cap="\\label{fig:temporal_tradeoff} Absolute information gain plotted by the optimal number of assessments, for students either grouped by ability or randomly assigned to classes. Colors show different target concepts, and dashed lines show the best fitting quadratic."}

ig_maxes <- d %>%
  filter(exponent == 1, knowledge == "uncertain") %>%
  group_by(teacherMu, grouping, numAssessments) %>%
  summarise(IG = mean(IG)) %>%
  summarise(numAssessments = numAssessments[IG == max(IG)[1]],
            IG = max(IG))

ggplot(ig_maxes, 
       aes(x = numAssessments, y = IG)) + 
  # geom_jitter( width = .2, height = .2)+ 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se=FALSE, col = "black", lty = 2) +
  geom_point(aes(col = factor(teacherMu)), size = 4) + 
  facet_grid(. ~ grouping) + 
  ggthemes::scale_color_solarized(name="Target Concept") + 
  xlab("Optimal number of assessments") +
  ylab("Information Gain (nats)") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") 
```


## Decision-Theoretic Applications: A Handful of Teachers and Assessments is Typically Optimal

In our final set of simulations, we consider an additional factor in making decisions: the costs and payoffs of different interventions.
<!-- We first consider how cost structures can be assigned to decisions and discuss different cost regimes. This analysis suggests that across a wide range of circumstances, a small number of assessments and a small number of teachers together lead to the majority of gains in learning performance.  -->
<!-- We also consider value functions that maximize students' learning differently, including a "remedial" value function that privileges students who have the most to learn. Under this value function, assessment is more valuable, because proper identification and class assignment for remedial students is critical. -->
Interventions to change class size are costly: they require resources to be allocated to hire more teachers. By assigning a notional "cost" $C_T$ (in arbitrary units) to teachers, it is possible to define a Pareto frontier -- a set of maximally efficient parameter settings -- between cost and information gain.

```{r fig.cap="\\label{fig:pareto_teacher} Information gain by cost of number of teachers, with colors showing the number of assessments for each parameter setting. The optimal assessment value for each cost is outlined and connected with a black line to show the Pareto frontier. Subplots show different levels of teacher knowledge and ability grouping. Target concept shown is .6."}

ig_by_cost <- d %>%
  filter(teacherMu == EXAMPLE_TEACHERMU, knowledge == "uncertain", grouping == "by ability") %>%
  mutate(cost = numTeachers) %>%
  group_by(knowledge, grouping, cost, numTeachers, numAssessments) %>%
  summarise(IG = mean(IG)) 


maxes_per_cost <- ig_by_cost %>%
  group_by(knowledge, grouping, cost, numTeachers) %>%
  summarise(max_IG = max(IG)[1],
            numAssessments = numAssessments[IG == max_IG][1])
  

ggplot(ig_by_cost,
       aes(x = numTeachers, y = IG, fill = numAssessments)) + 
  geom_point(col = "white", pch = 21, size = 4, alpha = .5) + 
  geom_line(data = maxes_per_cost, 
             aes(y = max_IG)) + 
  geom_point(data = maxes_per_cost, 
             aes(y = max_IG), 
             pch = 21, size = 4, alpha = 1, col = "black") + 
  # geom_text(aes(label = numAssessments), nudge_y = .6) +
  # geom_line() + 
  # facet_grid(grouping~knowledge)+
  ylim(c(0,200)) + 
  scale_fill_gradient2(name = "Number of Assessments", midpoint = 6) + 
  scale_x_continuous(breaks = c(1,2,3,5,10)) + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") + 
  xlab("Cost (Number of Teachers)") + 
  ylab("Information Gain (nats)")
```
Figure \ref{fig:pareto_teacher} shows an example Pareto frontier for one target concept, assuming both teachers with uncertainty about students and assignment to classrooms by ability. As described above, for ability grouping, adding a second classroom improves performance markedly, but returns diminish with additional classrooms. In contrast, when students are randomly assigned to classes, increasing the number of teachers helps only marginally and a small number of assessments serves to maximize information gain within each classroom. 


```{r fig.cap="\\label{fig:pareto_regimes} Pareto frontiers for three example cost function regimes (panels).  Line types show a range of target concepts, and point size shows the number of teachers. Other plotting conventions are as in Figure \\ref{fig:pareto_teacher}."}

pareto <- function(settings) {
  d %>%
    filter(teacherMu == settings$teacherMu, exponent == settings$exponent, 
           knowledge == "uncertain", grouping == "by ability") %>%
    mutate(cost = numAssessments * settings$C_A + numTeachers * settings$C_T) %>%
    group_by(knowledge, grouping, cost, numTeachers, numAssessments) %>%
    summarise(IG = mean(IG)) %>%
    ungroup %>%
    arrange(cost) %>%
    filter(IG == cummax(IG)) %>%
    mutate(regime = settings$regime, 
           teacherMu = settings$teacherMu,
           exponent = settings$exponent)
}


regimes <- data_frame(exponent = 1, 
                      C_A = c(500,250, 50),
                      C_T = c(50,250, 500), 
                      regime = c("Assessment Costs Dominate","Equal Costs","Teaching Cost Dominates")) %>%
  merge(data_frame(teacherMu = c(.5, .6, .7))) %>% # makes use of outer join behavior from merge
  mutate(index = 1:n()) %>%
  split(.$index) %>% 
  map_df(pareto)
  
ggplot(regimes,
  # filter(regimes, regime == "Teaching dominates"),
       aes(x = cost, y = IG, fill = numAssessments, lty = factor(teacherMu))) + 
  geom_line() + 
  geom_point(aes(size = numTeachers), col = "black", pch = 21) +
  # geom_point(pch = 21, size = 4, alpha = 1, col = "black") + 
  # geom_text(aes(label = numAssessments), nudge_y = .6) +
  # geom_line() + 
  facet_wrap(~regime)+
  # ylim(0,200) + 
  # xlim(0,2000) + 
  scale_size_continuous(name = "Number of Teachers", breaks = c(1,2,3,5,10))+ 
  scale_linetype_discrete(name = "Target Concept") +
  scale_fill_gradient2(name = "Number of Assessments", midpoint = 6, 
                       limits = c(0,12), breaks = c(0,6,12)) + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom", legend.box = "vertical") + 
  xlab("Cost (Arbitrary Units)") + 
  ylab("Information Gain (nats)")
```


Other cost structures are possible, however, leading to a number of different possible regimes. Assuming that assessments have a fixed cost $C_A$, due to classroom-exernal time or other resources expended in acquiring or creating them, then a number of regimes emerge. Figure \ref{fig:pareto_regimes} shows three sample regimes, in which assessment costs dominate ($C_A > C_T$), teaching costs dominate ($C_T > C_A$), or they are roughly similar ($C_A \approx C_T$), each shown for a range of target concepts. When assessment costs dominate, the most optimal choice is typically to hire many teachers. Interestingly, the converse is not true: even when teaching costs dominate, a quite small number of assessments is typically optimal. Across all simulations, most gains can be typically realized with a small number of assessments and a small number of teachers.^[This result mirrors a more general result due to @vul2014: under some sampling cost, a single sample from a binary variable -- in our case assessment -- is often enough for many decision problems.]

# General Discussion

### Class size effects

Very controverisial

@hoxby2000 reports very small effects
@mosteller1995 STARS reports larger effects. 

without coming to definitive judgment on the literature, we can say that expected effect sizes are probably relatively small. 

(discuss populations farther from the mean)

### Limitations

* motivation
* class dynamics
* teaching game we choose
* division of assessment and teaching, when actually teaching is informative


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

