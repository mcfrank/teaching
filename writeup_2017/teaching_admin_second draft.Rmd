---
title: "Optimal Models for Resource Allocation in Classroom Teaching"
bibliography: teaching.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Larry Liu} \\ \texttt{hrlarry@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    "Teaching is the transmission of knowledge from teacher to student. 
    How should a school administrator allocate a fixed budget towards increasing the number of classrooms and increasing student assessment to best increase student learning? This paper investigates how stochastic models accounting for the inherent uncertainty in student beliefs and teacher communication in teacher-student dyads can help school administrators figure out how to maximize student learning by optimally allocating resources. We replicate existing results from edu- cation literature about the effect of class sizes, homogenous ability classrooms, and assessment on student learning using computer simulation. We also unify these different determi- nants of student learning into a more holistic model and report the tradeoffs that committing budget to these design features presents. We demonstrated that we can create a pareto frontier for the number of teachers and assessments against a fixed bud- get, and find an optimal allocation of the budget for increasing student learning. We also demonstrate how the effectiveness of class sizes and assessment is contingent upon the learning concept being taught. We believe that the insights about stu- dent learning in multi-classroom settings that we've found and can continue to surface are difficult to surface in real-world studies with human subjects."
    
keywords:
    "Teaching; learning; education; pragmatics; Bayesian modeling; social cognition"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, out.width="3.25in", out.height="2.6in", fig.width=5, fig.height=4, fig.crop = FALSE, fig.pos = "t", fig.path='figs/', echo=FALSE, warning=FALSE, cache=TRUE, message=FALSE, sanitize = TRUE)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(directlabels)
library(forcats)
library(cogsci2016) #requires devtools::install_github("kemacdonald/cogsci2016")
library(langcog) #requires devtools::install_github("langcog/langcog")
```

<!--Writing Goals:
*Avoid using the phrase "learning concept" -- what does that even mean?
*Commit to either "students", "learners", or both, but make explicit
*Which is the primary motivation for this research? The fact that the ambiguity lends itself well to stochastic modeling, or because we need to use modeling to test different regimes?
-->

Education research has surfaced many insights into policies for designing schools to improve classroom learning for students. Very prominently, conventional wisdom makes independent claims that decreasing class sizes, increasing formative assessments, and increasing teaching periods *[citation needed for last one]* improves student learning outcomes. However, decreasing class sizes and developing formative assessments compete for a shared resource of money, while administering formative assessments and teaching lessons compete for a shared resource of student time. As such, it is important to understand the diminishing returns for each of these orthogonal design dimensions in order to find a unified policy that is optimal for student learning. We call the optimal policy **optimal school administration**.

Unfortunately, there exists many barriers to studying optimal school administration in real-world classrooms. Isolating the effects of a chosen school design policy would require controlling for student and teacher differences, and no two classrooms, teachers, and students are the same. Even if we could control for these sources of random error, the task of testing hundreds of permutations across multiple policy dimensions would be time- and cost-prohibitive, along with being ethically suspect in some extreme cases. As such, the task of determining optimal school administration lends itself well to computer modeling.

In this present paper, we turn to stochastic modeling to investigate how an optimal school administrator might allocate finite resources to produce maximal information gain. We elect to utilize stochasticity to build inherent student variability and error on assessments in the real world into our model. Taken together, this work describes a first-principles attempt at a framework for understanding resource allocation in classroom education.

# Background

Even when students are motivated to learn, and teachers are motivated to teach, information transmission in classroom settings are imperfect. Education in classroom settings has two challenges that we seek to tackle. First, there exists a problem of *student variability*, where students within the same classroom may have different innate academic ability as well as upbringings. This means that a lesson that would be perfect for one child may be less accessible to another child. Secondly, there exists a problem of *imperfect teacher knowledge*. Teachers don't have perfect knowledge of each student's personal knowledge, and this uncertainty potentially results in choices of teaching exmaples that are not optimal for student learning.

## Definitions

Because our work builds on some fields of research that have different vocabularies for similar concepts, we first clarify the language and meanings that will appear in this paper.

### Types of knowledge

Every student possesses their own malleable *personal knowledge* about any knowledge concept, which could be facts, skills, beliefs, or any other type of knowledge. Coloquially, personal knowledge is often referred to as ability or mastery level, and we use these terms interchangeably.

Teachers can identify a static *target knowledge* about any knowledge concept, and a student's personal knowledge can be close to or far from the target knowledge, representing accuracy or inaccuracy respectively. The goal of education, therefore, is to adjust the student's personal knowledge to be as close to the target knowledge as possible.

Teachers possess beliefs about what their students' personal knowledge is, which we will call *teacher beliefs*. These may not necessarily accurately reflect their students' actual personal knowledge.

### Education processes

Assessing, teaching and learning are the three key component processes of education. *Assessing* involve students providing information to teachers to *assessments* (e.g. assignments, quizzes, exams, classroom participation, etc.) based on their personal knowledge. The answers provided help teachers update their teacher beliefs. 

*Teaching* involves a teacher providing *lessons* to students to help adjust their students' personal knowledge towards the target knowledge. The content of the lessons are determined by the teacher beliefs; in other words, the teacher chooses *teaching examples* that they believe are most suitable given their students' personal knowledge.

Finally, *learning* involves a student using the teaching examples provided by the teacher through teaching to update their own personal knowledge. Learning itself does not guarantee that a student will update their personal knowledge to be closer to the target knowledge; the student must rely on the teacher to pick effective teaching examples for the updates to be useful.

### Example -- TODO: Keep this section?

A teacher wants to teach the effectiveness of nonviolent protest--the *target knowledge* is that nonviolent protests are somewhere between always successful and never successful. There is a student who holds the (inaccurate) *personal knowledge* that nonviolent protest is always unsuccessful. While there indeed are instances of failed nonviolent protests (e.g. Tianenmen Square, The White Rose), a teacher may elect to over-represent successful nonviolent protests in their *lessons*, using Martin Luther King Jr., Gandhi, and Nelson Mandela as *teaching examples*.

## Related Work

### Education Research

Educators have a variety of strategies to address the problems of student variability and imperfect teacher knowledge. Formative assessments can help teachers monitor their students' personal knowledge. When students complete formative assessments, teacher gain certainty on their teacher beliefs about their students' personal knowledge [e.g., @fuchs1986;@sadler1989]. If there is large variation in students' ability levels, students can be sorted into groups by teacher beliefs about their personal knowledge and taught different lessons [e.g. @slavin1987;@tomlinson1999]. Finally, decreasing class sizes can help minimize the average variation that each teacher has to deal with in their classroom [e.g., @glass1979;@slavin1989]. Each of these strategies are considered effective ways to improve student learning.

### Classroom Modeling

In our previous work, we conceptualized the teacher's task as one of optimal communication [@frank2014]. Following the models of pragmatic reasoning in language comprehension [@goodman2016;@frank2012], we can formalize teaching and learning as inferential cognitive processes of rational agents. Teachers reason about what evidence would best change students' personal knowledge to more closely correspond to a target knowledge. Teachers -- each with perfect teacher knowledge about each of their students in our prior work -- would then choose the teaching examples that maximized information gain across their students. Using this conceptualization, we were able to derive a number of results through simulation. For example, we found that individual student outcomes were inversely related to class size, since in smaller classes, teachers could customize their teaching better to the idiosynrasies of their particular group of students' personal knowledge.^[Ability grouping has a complicated history in education [e.g., @slavin1990], and we return to  motivational issues related to this finding in the General Discussion.]

In that previous work and the current work, the fundamental unit of analysis is a teaching game. In each teacher-class unit, a teacher tries to guide the students to discover a particular target knowledge by presenting teaching examples. We use a very simple concept: the weight of a biased coin. The target knowledge is a particular fixed weight, and the students' personal knowledge was their individual beliefs about what the weigh tof the coin is. Teachers must choose a particular set of teaching examples of heads and tails that will alter the personal knowledge of the students in the class so that their updated estimate of the coin weight is closer to the teacher's target knowledge value.

## The Present Study

In the current work, we consider issues of student variability and imperfect teacher knowlege through the lens of *optimal school administration*. We describe a generalization of the model presented in @frank2014 and use this framework to investigate how an optimal adminstrator might make decisions. Our first two simulations replicate and extend results from the previous paper. Then our next two generalize the model to the case of imperfect information about students and finite resources for hiring teachers and conducting assessments. Our final simulation maps out a Pareto frontier for allocation of instructional time and teaching resources.

<!---Old stuff--->
<!--
 
Teaching is the process of knowledge transmission from teachers to students. A teacher attempts to provide information to students -- whether facts, skills, generalizations, or another type of knowledge -- in a form that will be most likely to result in the student learning. But even when students are motivated to learn, and teachers are motivated to teach, information transmission in the classroom is imperfect. Classroom teachers must teach to students with different abilities and backgrounds, making the perfect lesson for one child less accessible to another (the problem of *student variability*). Adding to this issue, teachers don't have perfect knowledge of what students know or how they learn (the problem of *imperfect knowledge*). 

Educators have a variety of strategies to address these problems. If students are too diverse in their knowledge, they can be grouped by ability and provided with different lessons [e.g., @slavin1987;@tomlinson1999]. And formative assessments -- tests that reveal a student's starting state -- can help teachers know what level students start at [e.g., @sadler1989;@fuchs1986]. But breaking students up into groups or separate classes is resource intensive and can be inefficient -- in the limit, it would be impossible to give every single student a separate tutor, even if it might learn to better learning. And every assessment has a cost in terms of lost instructional time. When should educators use these tools? The goal of the current paper is to provide a formal analysis of these questions. We refer to this process as the process of "optimal school administration."

In our previous work, we conceptualized the teacher's task as one of optimal communication [@frank2014]. Following models of pragmatic reasoning in langauge comprehension [@goodman2016;@frank2012], we modeled teachers as reasoning about what evidence would best change student beliefs to more closely correspond to a target. Teachers -- with perfect knowledge about each of their students -- would then choose the learning example that maximized information gain across their students. Using this conceptualization, we were able to derive a number of results through simulation. For example, we found that individual student outcomes were inversely related to class size, since in smaller classes, teachers could customize their teaching better to the idiosynrasies of their particular student group.^[Ability grouping has a complicated history in education [e.g., @slavin1990], and we return to  motivational issues related to this finding in the General Discussion.]

In that previous work and the current work, the fundamental unit of analysis is a teaching game. In each teacher-class unit, a teacher tries to guide the students to discover a particular concept by presenting examples. We use a very simple concept, the weight of a biased coin. Teachers must choose particular examples of heads and tails that will alter the beliefs of the students in the class so that their updated estimate of the coin weight is closer to the teacher's target concept. 

In the current work, we consider issues of student variability and imperfect teacher knowlege through the lens of *optimal school administration*. Given finite resources, what arrangement of teachers and students, and assessements and lessons, produces the greatest information gain? We describe a generalization of the model presented in @frank2014 and use this framework to investigate how an optimal adminstrator might make decisions. Our first two simulations replicate and extend results from the previous paper. Then our next two generalize the model to the case of imperfect information about students and finite resources for hiring teachers and conducting assessments. Our final simulation maps out a Pareto frontier for allocation of instructional time and teaching resources. Taken together, this work describes a first-principles attempt at a framework for understanding resource allocation in classroom education.

-->

# Model

We model three types of agents: *students*, *teachers*, and *administrators*. A school consists of an administrator, at least one teacher, and at least one student. Every teacher has at least one student (so there are always at least as many students as teachers). Each agent's functions are described below.

The general teaching game that we analyze is one in which learners must estimate the parameter of a Bernoulli distribution. Teaching lessons are the results of individual coin flips, which provide evidence about the coin's weight. Beliefs can then be represented as the parameters of a Beta distribution. For example, a student who has a weak belief that a coin is fair (e.g., $Beta(1,1)$) can be pursuaded that it is actually biased towards heads by seeing the examples $E = \{H, H, H, H, H\}$. 

## Student

Each student is an optimal Bayesian learner, using a standard conjugate Beta-Bernoulli model. Students each have their own *prior personal knowledge* about the bias of the coin, represented as $Beta(\alpha,\beta)$, where $\alpha$ and $\beta$ are "pseudocounts" of heads and tails respectively. Students' learning is then modeled as updating this distribution by adding observed counts to their priors, e.g. after observing $x$ heads and $y$ tails in the teaching examples shown by the teacher, their updated *posterior personal knowledge* state is $Beta(\alpha + x, \beta + y)$. Note that for simplicity, student learning is sequence-independent (i.e., seeing $\{H, T, T\}$ is the same as seeing $\{T, T, H\}$).

Note that student personal knowledge distributions are generated by sampling $\alpha \sim Unif(1,10)$ and then setting $\beta = 11 - \alpha$ (so that total pseudocounts sum to 11). We intentionally hold constant the sum of pseudocounts because it controls how strongly each teaching example affects the student  posterior personal knowledge -- for example, a student who has a prior personal knowledge distributed as $Beta(10,10)$ will have their posterior personal knowledge swayed much more heavily compared to a student whose prior personal knowledge is distributed as $Beta(30,30)$ when seein ${H, H, H, H, T}$.

## Teacher 

Each teacher is assigned a classroom of students and a target knowledge concept (i.e., a particular coin weight) to teach. The goal of the teacher is to provide the set of teaching examples that maximize the student's information gain (IG; defined below). This goal is accomplished by evaluating the information gain for each student for each possible set of teaching examples and choosing the one that produces the largest total information gain for the class.^[A fruitful direction for future work would be to investigate different classroom rules for information gain. For example, a teacher following a remedial policy could try to find the set of examples that maximized the performance of the lowest-performing students or that minimized loss with respect to some threshold.] Since information gain is computed over the conjugate posterior personal knowledge of the students, choosing an action relative to IG constitutes full posterior inference. With only a single teaching example, this choice is simply the ratio of the IG for $H$ to the IG for $T$. 

<!-- **TODO: change here if simulation numbering changes** -->
In Simulations 1 and 2, teachers have *perfect teacher beliefs* of student beliefs. Teachers with perfect teacher beliefs infer their choice of examples using the exact parameters of student distributions. In contrast, the teachers in the remaining simulations have *uncertain teacher beliefs*. Teachers with uncertain knowledge initially represent students as having beliefs in the form of $Beta(1,1)$ -- weak uniform distributions over possible parameter values. They update these representations based on *assessments*. For example, if a student was given three assessments and produced $\{H, T, H\}$, the teacher would represent that student as $Beta(1+2,1+1) = Beta(3,2)$ and choose examples to accordingly. Assessments are sampled examples from a Bernoulli distribution parameterized by each student's mean parameter estimate, using $\mu = \frac{\alpha}{\alpha + \beta}$. Teachers integrate the samples from these assessments into their distributional estimate for each student. 

## Administrator 

The objective of the administrator is to maximize the information gain of all students in the school. Across our simulations, the administrator can decide: 1) how many teachers to hire, 2) how many assessments to give, and 3) whether to sort students into classrooms by their knowledge. We also vary (for purposes of comparison) whether the teachers have perfect or uncertain teacher beliefs of the students' personal knowledge, though in real-world settings teachers only have uncertain beliefs. The administrator weights the various policies that they simulate based on the aggregate information gain of the students in the entire school (compared to just each classroom for each teacher's inference), and is able to infer the most effective school design within fixed constraints. As in the case of teachers, in Simulations 1 and 2, administrators also have perfect knowledge of their students' personal knowledge. In contrast, in Simulations 3 and 4, administrators sort students based on the results of their assessments, the same information with which teachers use to choose examples.

In practice, because our results demonstrate near-strict dominance of some design choices over others, the administrator's inference is often uninteresting and corresponds to the best choice. Thus, in reporting our simulations, we report school-wide information gain (the decision-making metric for the administrator), often with respect to some meaningful baseline.

## Limited Resources

In our model, two types of resources restrict the behavior of the different agents.

### Time

In real-world education settings, students are in a school environment for a (roughly) fixed amount of time, and it is up to the teachers to decide what amount of that time they want to dedicate to assessing (i.e. formative assessments) and teaching (i.e. showing teaching examples). In Analysis 3 and 4, where we use "noisy" students, teachers have the option to assess students to improve their imperfect teacher beliefs about the students' personal knowledge. We model this using a resource we call fixed *time steps*. The sum of the number of *assessment periods* and *teaching periods* is constant, and increasing assessments comes at the expense of opportunities to show examples. The administrator commits to a particular allocation of time steps towards assessments and teaching lessons school-wide, so every teaacher presents the same number of teaching examples.

### Money

Certain educational policies that may be most effective are simply too monetarily expensive to implement. For instance, while one-to-one instruction (i.e. class sizes of 1) may be most beneficial for student learning outcomes [e.g., @cohen1982], it is unrealistic to expect schools to be able afford hiring as many teachers as they have students. To account for this kind of limitation, we introduce the resource of money into our model. In Analysis 4, the administrator is constrained by a fixed budget. This budget can be used to hire teachers at a one-time expense $C_T$ per teacher. We then assume an additional monetary cost per assessment $C_A$, corresponding to the costs of implementing a school-wide assessment. We report the optimal allocation of money at varying budget levels towards teachers and assessments.

## Information gain

Following @frank2014, we use information gain to quantify student learning. Formally, we assess the Kullback-Leibler divergence [@cover2012] between student knowledge ($B_S$) and the teacher's target distribution ($B_T$) both before and after teaching. The difference between these quantities gives the student's information gain for a single example $e$:

$$IG(e) = D_{KL}(B_T ||B_S) - D_{KL}(B_T || B_{S+e})$$

We derived a closed form expression for information gain that generalizes to any number of examples in an example set $e$. Derivation details can be found in our linked repository. The final form for $h$ heads and $t$ tails is:

\begin{multline}
IG(E) = \sum_{k=0}^{h+t-1} {log(\alpha_S + \beta_S + k)} - \\
\sum_{i=0}^{h-1} {log (\alpha_S + i)} - \sum_{j=0}^{t-1} {log(\beta_S +j)} + \\
\psi(\alpha_T)h + \psi(\beta_T)t +  \psi(\alpha_T + \beta_T)(h+t).
\end{multline}

\noindent where $\psi$ represents the digamma function, and $\alpha_S$, $\beta_S$, $\alpha_T$, and $\beta_T$ are student and teacher priors respectively. 

<!-- TODO: Does this terminology make sense? -->
## Belief Softmax and Belief Strength

As discussed earlier, we represent the target knowledge, student knowledge, and teacher beliefs about the student knowledge as Beta distributions $Beta(\alpha, \beta)$. Frank (2014) presented an alternative parameterization of the Beta-Bernoulli distribution in terms of shape $\mu$ and scale $\nu$ that is useful for its intuitive conceptual analogs[@frank2014]. $\mu$ and $\nu$ are defined as:

$$\mu = \alpha / (\alpha + \beta)$$
$$\nu = \alpha + \beta$$

In this parameterization, $\mu$ directly controls the mean of the distribution. In the target knowledge distribution, this can be regarded as the *true weight* of the coin. In the student knowledge distribution, $\mu$ represents the student's *best guess* for the true weight of the coin. In the teacher belief distribution, $\mu$ represents what the teacher believes the student's best guess for the true weight of the coin would be (i.e. given the teacher belief about the student knowledge). Since Beta-Bernoulli distributions are symmetric around $\mu = 0.5$ (e.g. for the same $\nu$, the probability distribution for $\mu = 0.3$ is the same as $\mu = 0.7$ flipped across 0.5), we only look at $\mu \geq 0.5$, where the true weight or best guess is more extreme for $\mu$ values further away from 0.5. In each case, $\nu$ captures the *belief strength* or confidence. The larger $\nu$ is, the greater the number of examples of Heads or Tails that would be necessary to alter the belief.

For example, a student with student knowledge $Beta(5,5)$ and another student with student knowledge $Beta(10,10)$ would both have a best guess that the coin weight is 0.5, i.e. $\mu$ = 0.5, but fewer teaching examples would be required to convince the former student that the true weight actually is not 0.5 than the latter because the former has weaker beliefs in that $\mu$ value at $\nu$ = 10 compared to the the latter's stronger beliefs at $\nu$ = 20. Similarly, one students with student knowledge $Beta(1,9)$ and another with student knowledge $Beta(9,1)$ have vastly different best guesses for the true weight of the coin ($\mu$ = 0.1 and $\mu$ = 0.9 respectively). However, because both share the same belief strengths at $\nu$ = 11, their beliefs about the coin's weight could be shifted to 0.5 with an equal number examples (8 tails for the former and 8 heads for the latter to achieve $Beta(9,9)$).

<!--

## Simulation details

For simplicity, we adopt a set of parameters uniformly across our simulations. While changes to these parameters will have some impact on the effect sizes we recover, to our knowledge all qualitative results are general across parameter sets. Teacher target concepts are selected via pseudocounts on a Beta distribution such that $\alpha + \beta = 10$ (thus target concepts can be .1, .2, .3, etc.).\footnote{These pseudocounts are slightly offset from the pseudocount of the prior student belief distributions, 11, to avoid edge cases where a student belief distribution perfectly matches the teaching concept.} Each student has 12 learning opportunity time steps that can be used for showing a teaching example or for assessment in later simulations.

In each simulation, we report the total information gain over baseline for 100 students given any particular school design. The particular design parameters (number of teachers, number of assessments, teaching concept, perfect vs. imperfect teacher knowledge, and sorting) and baseline configuration differs in each simulation. Each simulation is tested on 100 random sets (*trial*s) of 100 students and performance is averaged across these trials. All simulations were conducted using the probabilistic programming language \texttt{webppl} [@goodman2017]; code is available at [http://github.com/mcfrank/teaching].

-->

# Simulations

```{r}
NUM_STUDENTS <- 100
df <- read_csv("../sims_admin/cached_data/raw_data_041017_3Per11Nu_exp_large.csv")
#df <- read_csv("../sims_admin/cached_data/raw_data_053017_10timesteps_small.csv")
df2 <- read_csv("../sims_admin/cached_data/raw_data_050917_highResMus.csv")
studentByStudent_df <- read_csv("../sims_admin/cached_data/raw_data_052317_studentTracker.csv")
```

In this section, we present the hypotheses and results of each of our simulations. Each simulation consists of 100 *trial*s. Each trial involves generating 100 random students and measuring the aggregate information gain for those 100 students under the particular school design policy the simulation is testing. The performance of the school design policy is calculated as the average of the aggregate information gain across all 100 trials.

### General simulation details

All simulations were conducted using the probabilistic programming language \texttt{WebPPL} [@goodman2017]; the code is available at [http://github.com/mcfrank/teaching].

In our simulations, there is a set of three parameters that we hold constant across all experiments for simplicity. While changes to these parameters will have some impact on the effect sizes we recover, to our knowledge all qualitative results are general across parameter sets. 

* First, the target knowledge is always represented by a Beta distribution such that the parameters of $Beta(\alpha,\beta)$ are non-zero and sum to 10, i.e. $\alpha + \beta = 10$. We choose 10 because the calculation of the true weight of the coin $\mu$ are simply clean increments of one-tenths. Additionally, because the effects would be symmetric if $\alpha$ and $beta$ were swapped, we restrict $\alpha <= \beta$. That is, the target knowledge true weight is limited to [.5, .6, .7, .8, .9].

* Second, the prior student knowledge is always represented by a Beta distribution such that the parameters of $Beta(\alpha,\beta)$ are non-zero and sum to 11, i.e. $\nu = \alpha + \beta = 10$. This fixes the belief strength of every student's prior student knowledge -- every example shown by a teacher will carry the same weight for every student because the sum of the pseudocounts is always equivalent. The total pseudocounts of prior student knowledge distributions, $\nu$ = 11, is slightly offset from the pseudocount of the target knowledge distribution, $\nu$ = 10, to avoid edge cases where a student belief distribution perfectly matches the teaching concept.

* Third, each student has 12 learning opportunity time steps. As mentioned earlier, each time step can be uesd as a teaching period to show examples or, in the relevant simulations, as an assessment period to assess students. We choose 12 time steps because it is a sufficient number to effectively alter the beliefs of students with extreme prior personal knowledge (e.g. $Beta(1,10)$ can be changed all the way to $Beta(13,10)$), but does not become computationally prohibitive, as WebPPL explores all combinations in its inferences.
<!-- This feels like weak justification -->

<!-- Old stuff
We next present a set of four analyses of simulation data examining the effects of factors including grouping students into classe by their prior beliefs, changing class sizes, and the use of assessments to help teachers tailor their teaching to their specific students. In our first two analyses, we focus on the *perfect knowledge* case, in which teachers and administrators have full knowledge of students' knowledge state. In the subsequent two analyses, we relax this assumption and explore . -->

## Perfect Teacher Beliefs

In an ideal world, administrators and teachers have perfect teacher beliefs: they know the exact parameters of each students' personal knowledge distribution. As such, they can perfectly choose an optimal set of teaching examples that will maximize the total aggregate information gain of the students in their classroom. In addition, administrators can sort students into classrooms by their prior personal knowledge perfectly -- this ensures that teachers receive classrooms with as little variance in student personal knowledge as possible, making it easier to choose sets of teaching examples that are effective for all students in their respective classrooms. We use such a simulation environment with perfect teacher beliefs in our first three simulations (Simulation 1A, 1B, and 1C, details described below).

Running experiments in a simulation environment with perfect teacher beliefs can be an effective way to uncover the effects of certain aspects of school design policy because it removes sources of error that arise from teachers simply not having accurately measured students' prior personal knowledge. In doing so, we can ascertain that any effects on information gain are entirely accounted for by the school design policy. Additionally, in later simulations, we can use the student information gain outcomes in an environment with perfect teacher beliefs to benchmark the performance of school design policies in an environment with imperfect teacher beliefs. The outcomes in the environment with perfect teacher belief will serve as a strict maximal ceiling or "gold" standard -- the dropoff in information gain outcomes in an imperfect environment we can confidently attribute to inefficiencies that arise when teachers imperfectly infer students' personal knowledge.

It's worth explicitly noting that teachers having perfect teacher beliefs does not mean the set of teaching examples that they choose to use is optimal for each student. In many classrooms, there is no set of teaching examples that is unanimously optimal at the individual student level for every student. Rather, having perfect teacher beliefs enables teachers to pick sets of teaching examples with total certainty that it will be optimal at the classroom level, and there exists no better set of teaching examples that would further reduce the inefficiencies of choosing teaching examples for a varied set of students.

<!-- TODO: Change figure captions-->

### Simulation 1A: Grouping students
TODO: FIGURE OUT HOW TO LINEBREAK

```{r fig.cap="Student information gain, plotted by target concept and number of teachers in the school. Information gain represents the gain when students are sorted into classrooms based on knowledge compared with an unsorted baseline. Error bars show 95\\% confidence intervals by non-parametric bootstrap."}

df_spread <- spread(df, simType, IG)

df_sim1a <- df_spread %>%
  filter(numAssessments == 0, exponent == 1) %>%
  mutate(sorted = sortedPerfectTeachers, unsorted = unsortedPerfectTeachers) %>%
  select(-c(sortedPerfectTeachers, sortedUncertainTeachers, unsortedNaiveTeachers, unsortedPerfectTeachers, unsortedUncertainTeachers)) %>%
  gather(key = simType, value = IG, sorted, unsorted)

df_sim1a_means <- df_sim1a %>%
  group_by(numTeachers, teacherMu, simType) %>%
  multi_boot_standard(col = "IG")
  
# ggplot(df_sim1a, aes(x = factor(simType), y = IG)) +
#   geom_boxplot() + 
#   facet_grid(teacherMu ~ numTeachers)
# 
# ggplot(df_sim1a, aes(x = factor(simType), y = IG)) +
#   geom_boxplot() + 
#   facet_grid(teacherMu ~ numTeachers, scales="free_y")

ggplot(df_sim1a_means, aes(x = simType, y = mean, col = factor(teacherMu))) +
  geom_point() +
  facet_grid(teacherMu ~ numTeachers, scales="free_y") +
  ylab("Information Gain (nats)") + 
  theme(legend.position = "bottom") +
  xlab("Number of Teachers")


```

```{r, include=F}
sorted_over_unsorted <- df_spread %>%
  filter(numAssessments == 0, teacherMu != .9) %>%
  mutate(IG_over_baseline = sortedPerfectTeachers - unsortedPerfectTeachers) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "IG_over_baseline")

ggplot(sorted_over_unsorted, aes(x = numTeachers, y = mean, col = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() + 
  langcog::scale_color_solarized(name = "Target Concept") +
  ylab("Information Gain Over Unsorted (nats)") + 
  theme(legend.position = "bottom") +
  xlab("Number of Teachers") + 
  scale_x_continuous(breaks = c(1, 2, 3, 5, 10))

df_spread2 <- spread(df2, simType, IG)

#LOOK HERE - It appears that 0.62-0.64 is simply the most potent...?
sorted_over_unsorted2 <- df_spread2 %>%
  filter(numAssessments == 0, teacherMu != .9) %>%
  mutate(IG_over_baseline = sortedPerfectTeachers - unsortedPerfectTeachers) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "IG_over_baseline")

ggplot(sorted_over_unsorted2, aes(x = teacherMu, y=mean)) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  facet_grid(. ~ numTeachers) + 
  ggthemes::theme_few() + 
  langcog::scale_color_solarized(name = "Target Concept") +
  ylab("Information Gain Over Unsorted (nats)") + 
  theme(legend.position = "bottom") +
  xlab("TeacherMu") + 
  scale_x_continuous(breaks = c(.5, .52, .54, .56, .58, .6, .62, .64, .66, .68))
```

*Research Question.    *
How does grouping students by their true *prior personal knowledge* affect information gain?

*Design and Hypotheses.    *
In the manipulation (sorted) condition, the administrator sorts students into equally-sized (±1 student) classrooms by their prior personal knowledge. In the control (unsorted) condition, the administrator performs no sorting, randomly distributing the students into equally-sized classrooms. We hypothesized that sorting students by their prior personal knowledge would increase information gain compared to random classroom assignment because sorting would reduce variance in student personal knowledge within classrooms, allowing teachers to better tailor the teaching examples to that particular group of students.

We further hypothesized that there'd be an interaction effect between sorting and both the number of teachers and the extremity of the target knowledge concept. Specifically, we believe that sorting results in greater information gain as the number of teachers increases, since no matter how sorted the rosters are, having too many students in one class still prevents teachers from finding a set of teaching examples that will be effective for every student in the class. On the other hand, we believe sorting will be less effective for more extreme target knowledge concepts (higher $\mu$ values for the target knowledge distribution), since for extreme target knowledge concepts, most students will have best guesses that fall on the same side of the true weight of the coin, and so no matter what the prior personal knowledge distributions of the stduents are, the teacher can simply choose very biased teaching examples and they will be effective.

*Results.    *
Results are shown in Figure 1. Sorted students show greater information gain than if the same set of students are distributed into unsorted classrooms. This effect is present for all target concepts but is most pronounced for less-extreme concepts -- for extreme value concepts (e.g., a target of .9), almost all students will benefit from seeing the same examples anyway, rendering sorting irrelevant. Thus, an optimal school administrator with perfect student knowledge should consistently opt to sort students into classrooms by their prior beliefs over random assignment.


### Simulation 1b: Number of Teachers or Class Size
LINEBREAK

```{r, fig.cap="This figure should probably go in the appendix, but helps illustrate why using the baseline is useful"}

df_sim1b <- df_spread %>%
  filter(numAssessments == 0, exponent == 1) %>%
  select(-c(sortedUncertainTeachers, unsortedNaiveTeachers, unsortedPerfectTeachers, unsortedUncertainTeachers)) %>%
  mutate(class_size = NUM_STUDENTS / numTeachers) %>%
  group_by(numTeachers, teacherMu) %>%
  multi_boot_standard(col = "sortedPerfectTeachers")
  
# ggplot(df_sim1a, aes(x = factor(simType), y = IG)) +
#   geom_boxplot() + 
#   facet_grid(teacherMu ~ numTeachers)
# 
# ggplot(df_sim1a, aes(x = factor(simType), y = IG)) +
#   geom_boxplot() + 
#   facet_grid(teacherMu ~ numTeachers, scales="free_y")

ggplot(df_sim1b, aes(x = numTeachers, y = mean, col = factor(teacherMu))) +
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() + 
  langcog::scale_color_solarized(name = "Target Concept") +
  theme(legend.position = "bottom") + 
  ylab("Info Gain Over 1 Teacher (nats)") + 
  xlab("Number of Teachers") +
  scale_x_continuous(breaks = c(1, 2, 3, 5, 10))

```


```{r, fig.cap="Student information gain, plotted by target concept and class size. Information gain represents the gain when students are sorted into classrooms based on knowledge compared with a single-classroom baseline. Error bars show 95\\% confidence intervals by non-parametric bootstrap."}
sorted_by_teacher_num <- df_spread %>%
  filter(numAssessments == 0, exponent == 1) %>%
  mutate(class_size = NUM_STUDENTS / numTeachers) %>%
  select(-sortedUncertainTeachers, -unsortedPerfectTeachers, -unsortedUncertainTeachers) %>%
  group_by(teacherMu, trialNum) %>%
  mutate(IG_over_baseline = sortedPerfectTeachers - 
           sortedPerfectTeachers[numTeachers == 1]) %>%
  group_by(teacherMu, numTeachers) %>%
  multi_boot_standard(col = "IG_over_baseline")

ggplot(sorted_by_teacher_num, aes(x = numTeachers, y = mean, col = factor(teacherMu))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() + 
  langcog::scale_color_solarized(name = "Target Concept") +
  theme(legend.position = "bottom") + 
  ylab("Info Gain Over 1 Teacher (nats)") + 
  xlab("Number of Teachers") +
  scale_x_continuous(breaks = c(1, 2, 3, 5, 10)) 

# Below is class size measure rather than numTeachers measure
#
# sorted_by_teacher_num <- df_spread %>%
#   filter(numAssessments == 0, teacherMu != .9, exponent == 1) %>%
#   mutate(class_size = NUM_STUDENTS / numTeachers) %>%
#   select(-sortedUncertainTeachers, -unsortedPerfectTeachers, -unsortedUncertainTeachers) %>%
#   group_by(teacherMu, trialNum) %>%
#   mutate(IG_over_baseline = sortedPerfectTeachers - 
#            sortedPerfectTeachers[numTeachers == 1]) %>%
#   group_by(teacherMu, numTeachers) %>%
#   multi_boot_standard(col = "IG_over_baseline")

# ggplot(sorted_by_teacher_num, aes(x = class_size, y = mean, col = factor(teacherMu))) + 
#   geom_line() + 
#   geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
#   ggthemes::theme_few() + 
#   langcog::scale_color_solarized(name = "Target Concept") +
#   theme(legend.position = "bottom") + 
#   ylab("Info Gain Over 1 Teacher (nats)") + 
#   xlab("Class size") +
#   scale_x_continuous(breaks = c(10, 20, 33, 50, 100)) 

```

*Research Question.    *
How does the number of teachers, which is inversely proportional to the class size, affect student information gain?

*Design and Hypotheses.    *
In this simulation, the administrator changes the number of classrooms to distribute students. There can be 1, 2, 3, 5, or 10 classrooms, resulting in class sizes of 100, 50, 33/34, 20, and 10 respectively. We hypothesized that increasing the number of teachers would strictly improve student learning rate: just like with sorting, more teachers would reduce class sizes, which in turn reduces student variance within classrooms. This enables teachers to choose better tailored sets of learning examples.

<!-- TODO: Talk about interaction -->

*Results.    *
Results are shown in Figure 2. We observed the predicted pattern. Increasing the number of teachers increases the information gain. although there were diminishing returns. After a certain number of teachers, additional lesson customization becomes less helpful.

In our second analysis, again a replication of our prior work, we explored the effects of adding teachers to the simulated school, leading to lower class sizes. We hypothesized that increasing the number of teachers would strictly improve student learning rate (again assuming perfect knowledge about students):  Our baseline for this analysis was the sorted information gain under the same target bias parameters. 


## Simulation 1c: Prioritizing different students



```{r}
sorted_by_exponent <- df_spread %>%
  filter(numAssessments == 0, teacherMu != .9) %>%
  select(-sortedUncertainTeachers, -unsortedPerfectTeachers, -unsortedUncertainTeachers) %>%
  group_by(teacherMu, trialNum) %>%
  mutate(IG_over_baseline = sortedPerfectTeachers^(1/exponent) -
           sortedPerfectTeachers[numTeachers == 1]) %>%
  group_by(teacherMu, exponent, numTeachers) %>%
  multi_boot_standard(col = "IG_over_baseline")


##This graph doesn't work very well.
ggplot(sorted_by_exponent, aes(x = exponent, y = mean, col = factor(exponent))) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() + 
  langcog::scale_color_solarized(name = "Exponent") +
  theme(legend.position = "bottom") + 
  ylab("Info Gain Over 1 Teacher (nats)") + 
  xlab("Class Size") +
  scale_x_continuous(breaks = c(10, 20, 33, 50, 100)) 

```

*Research Question.    *
Placeholder

*Design and Hypotheses.    *
Placeholder

*Results.    *
Placeholder

In our third analysis, we try to capture what in academic circles is called the proficiency vs. growth debate. We do so by varying the teachers' heuristics for selecting an example. A higher exponent term means that the teacher prioritizes sets of examples that result in worse-performing students making large gains at the expense of students who are already very close to the target knowledge making small gains. 

## Imperfect Teacher Beliefs

In our second set of analyses, we relax the assumption that teachers have perfect teacher beliefs about students' prior personal knowledge. In the real world, neither the admin nor the teacher is an omniscient being that knows the true prior personal knowledge parameters. Instead, they diagnose student beliefs by administering formative assessments (e.g. placement exams). The assessment phase is a necessary step that enables the administrator and teachers to form teacher beliefs about what the students' prior personal knowledge may be -- these beliefs are not perfect, but will be correlated with the students' personal knowledge. Only after these assessments are conducted can the administrator effectively sort students. The teachers can also then select examples guided by these teaching beliefs.

To model these agent behaviors, we assume that the admin and teachers start with teacher beliefs that are a na\"ive, uniform representation of each student (i.e. $Beta(1,1)$) and learn about the student's prior personal knowledge via the administration of assessments. In assessments, students are called upon to demonstrate their knowledge by sampling from their own true prior personal knowledge distribution. These samples then serve to update the teacher's estimate about student knowledge. In this sense, the teachers and the administrator are modeled as perfect Bayesian agents that update their teacher beliefs about student personal knowledge based on evidence it sees from student performance on assessments.

For instance, a student may have a true prior personal knowledge distribution represented by $Beta(9,2)$. When assessed, they are extremely likely to respond with $H$ (or $1$) to each question on the assessment. If they are asked 6 questions, the most common outcome will be ${H, H, H, H, H, T}$ (in some order). The teachers and administrator starts with the naive $Beta(1,1)$ representation of the student personal knowledge, but then updates the representation for seeing 5 heads and 1 tail in the assessment phase. Their *posterior teacher belief* (i.e. (post-assessment) of student learning becomes $Beta(6,2)$ after the Bayesian update. By construction, our uncertain teachers have very weak and inaccurate beliefs about student personal knowledge, captured by the low $\nu$ of the initial Beta representation, and we assume that increasing assessments will help them improve the accuracy and belief strength of their representation of student personal knowledge.

Given our model, increasing the number of assessments should monotonically improve student learning on average because the teachers get more accurate and precise teacher beliefs about the students' prior personal knowledge through assessments. As such, simply measuring information gain as the number of assessments increases is rather trivial. In the following simulation, we introduce the limited resource of time--each student spends a fixed number of 12 time steps in the school system, and any single time step can be devoted to assessing the student (3 samples from the student's prior persanal knowledge distribution, as described above) or a teacher showing the student one teaching example (a heads or a tails). By modeling the tradeoff between increasing assessments and increasing teaching opportunities, we attempt to identify a tipping point at which giving teachers more opportunities to show examples outweight the diminishing returns on information gain of increasing assessments, and vice versa.

In a simulation environment where imperfect teacher beliefs exist, inefficiencies are introduced in two ways. First, administrators may commit errors in sorting students into classrooms if they have an inaccurate guesses about the mean of students' prior personal knowledge distributions. Secondly, teachers may choose less efficient sets of teaching examples based on their guesses about their students' prior personal knowledge that may not actually optimally maximize information gain given the students' true prior personal knowledge.

Our previous work on modeling classroom dynamics [@frank2014] did not have any stochasticity in the generation of student distributions nor the inferences that teachers make about students' prior personal knowledge. We believe that by capturing the errors in the inferences and the formative assessments that teachers might use to reduce those errors in our model, our simulations are a novel contribution to the literature that more accurately resembles real-world classroom dynamics.

### Baseline

In Simulation 1a-1c, we observed that the information gain values vary significantly across different target knowledge concepts, or the true weight of the coin $\mu$. In real-world classroom settings, the target knowledge concept is an independent variable that are entirely orthogonal to the school design tradeoffs we are considering -- if high school students are trying to learn trigonometry, trigonometry must be the target knowledge regardless of whether its few or many teachers teaching, regardless of whether or not the students are sorted by ability level, and regardless of whether few or many formative assessments are conducted. As such, we want to control for the target knowledge concept by measuring *improvement* to information gain *within* levels of the target knowledge concept.

To do so, we introduce a baseline paradigm that uses a non-inferential admin and teacher that we consider the control condition. The baseline design involves no student sorting, using a single teacher, and no assessments (i.e. 12 teaching examples). This admin and teacher does not take their students' prior beliefs into consideration. Instead, they simply sample a Binomial distribution with a bias equal to the $\mu$ value from the target knowledge distribution to generate their example sets -- this is equivalent to actually flipping a coin of the weight $\mu$. As such, there would be a unique average baseline for each level of the target knowledge concept, but otherwise utilize the most basic design along the sorting, number of teachers, and number of assessments dimensions. We call this baseline control condition the *naive teacher* policy.

For instance, to generate 12 examples when the target knowledge concept is 0.7 in any one trial, the teacher would sample from $Binomial(n=12, p=0.7)$, with 9 Heads and 3 Tails being most likely, 8 Heads and 4 Tails being next most likely, and so forth. They would then show this set of teaching examples to every student in their classroom without consideration for their prior personal knowledge and whether or not that teaching example would actually be useful given their prior personal knowledge.

### Simulation 2: Imperfect Teacher Beliefs

As we consider imperfect teacher beliefs, we attempt to unify some of the different dimensions from Simulation 1 of sorting vs. unsorting, and the number of teachers while incorporating the new dimension of the number of assessments. Each of these dimensions may interact with the other, and so unlike in Simulations 1a-1c, we first report the full set of results before we slice the dataset and discuss each dimension in greater detail.

```{r sim_noisy_students, fig.cap="Information gain plotted by number of assessments (out of 12) for teachers with perfect and uncertain student knowledge.", fig.width=5.9, fig.height=3.1, fig.env = "figure*"}

ig_by_sim_type <- df %>%
  filter(exponent == 1.0) %>%
  group_by(numAssessments, numTeachers, teacherMu, trialNum) %>%
  mutate(sorted_perfect = IG[simType == "sortedPerfectTeachers"] - 
           IG[simType == "unsortedNaiveTeachers"], 
         sorted_uncertain = IG[simType == "sortedUncertainTeachers"] - 
           IG[simType == "unsortedNaiveTeachers"], 
         unsorted_perfect = IG[simType == "unsortedPerfectTeachers"] -
           IG[simType == "unsortedNaiveTeachers"],
         unsorted_uncertain = IG[simType == "unsortedUncertainTeachers"] -
           IG[simType == "unsortedNaiveTeachers"]) %>%
  gather(sim_type, IG_over_baseline, 
         unsorted_uncertain, sorted_uncertain, sorted_perfect, unsorted_perfect)
  

df_sim2 = ig_by_sim_type %>%
  group_by(numAssessments, numTeachers, teacherMu, sim_type) %>%
  summarise(mean = mean(IG_over_baseline)) %>%
  separate(sim_type, into = c("sorting","knowledge"))
  # mutate(sim_type = fct_recode(sim_type, 
  #                              "Unsorted, Uncertain" =
  #                              "Sorted, Perfect" = "sorted_perfect",
  #                              "Sorted, Uncertain" = "sorted_uncertain", 
  #                              "Unsorted, Perfect" = "unsorted_perfect"))
  #        # multi_boot_standard(col = "IG_over_baseline")

ggplot(df_sim2,
       aes(x = numAssessments, y = mean, col = sorting, lty = knowledge, pch = knowledge)) +
  geom_line() +
  geom_point() +
  facet_grid(teacherMu~numTeachers) +
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme(legend.position = "bottom") +
  ggthemes::theme_few() +
  langcog::scale_color_solarized() +
  ylab("Info Gain Over Naive Baseline (nats)") +
  xlab("Number of Assessments") +
  # scale_linetype(guide=FALSE) +
  # geom_dl(aes(label = sim_type), method = "smart.grid") +
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12)) + 
  geom_hline(yintercept = 0, lty = 2, col = "black")
```

```{r}
#TODO: Determine if this needs to be in there.
df_lm <- ig_by_sim_type %>%
  separate(sim_type, into = c("sorting","knowledge"))

model <- lm(IG_over_baseline ~ numTeachers + sorting * teacherMu * numAssessments, df_lm)
anova(model)

```

### Simulation 2a: Sorting
LINEBREAK

*Research Question.    *
Do the effects of sorting students by their prior personal knowledge found in Simulation 1a persist when the teachers have imperfect teacher knowledge? How do the effects compare with when the teachers have perfect teacher beliefs?

*Design and Hypotheses.    *
We use a 2 (sorted vs. unsorted) x 2 (perfect vs. imperfect teacher beliefs) experimental design. The sorted x perfect condition and unsorted x perfect condition are a replication of Simulation 1a's manipulation and control conditions respectively. In the sorted x imperfect condition, the administrator sorts students into equally-sized (±1 student) classrooms by the *teacher beliefs* about the prior personal knowledge, which are correlated with the prior personal knowledge itself due to the assessment phase but not necessarily identical. In both the sorted and unsorted imperfect conditions, teachers choose teaching examples that would be optimal given the teacher beliefs about the prior personal knowledge.

We hypothesized that sorting students into classrooms using imperfect teacher beliefs would still result in greater information gain than random student assignments. However, we suspected that the information gain achieved after sorting with imperfect teacher beliefs would be lower than when the students were sorted by their true prior personal knowledge because there may be sorting errors when using the imperfect teacher beliefs.

*Results.    *
As we predicted, sorting students produced a main effect on learning outcomes that were strictly superior than not sorting in both the perfect teacher belief condition (consistent with Simulation 1a) as well as the imperfect teacher belief condition. Just as in Simulation 1a, the effect of sorting interacts with number of teachers, where sorting has no effect when there is only 1 teacher because there is only 1 classroom (leftmost column in Figure 4). Similarly, the effect of sorting also interacts with target knowledge concept as it did in Simulation 1a, where sorting becomes less effective for more extreme target knowledge concepts (greater difference between red sorted IGs and blue unsorted IGs the higher the row in Figure 4).

Perhaps surprisingly, sorting students by the imperfect teacher beliefs still vastly outperforms unsorted school designs even when the teachers have perfect teacher beliefs and can pick the most optimal teaching examples within classrooms. This underscores the dominance of sorting policies. Without sorting, no matter how much an administrator increases the number of teachers or or assessments, students do not appear to gain substantially more information, because each classroom on average is still a representative microcosm of the entire set of students.

Given these results, we assume the optimal school administration policy uses student sorting, and we discuss all subsequent simulation analyses in terms of their information gain when students are sorted.

### Simulation 2b: Number of Teachers / Class Size
LINEBREAK

*Research Question.    *
Do the effects of the number of teachers by their prior personal knowledge found in Simulation 1b persist when the teachers have imperfect teacher knowledge? How do the effects compare with when the teachers have perfect teacher beliefs?

*Design and Hypotheses.    *
We use a 5 (1, 2, 3, 5, and 10) x 2 (perfect vs. imperfect teacher beliefs) experimental design. Based on our results from Simulation 1b, we hypothesized that increasing the number of teachers would strictly improve student learning rate because each classroom will be smaller, allowing teachers to pick more tailored teaching examples for their classroom. We also hypothesized that the improvement to information gain would have diminishing returns as the number of teachers increases, just as we found in Simulation 1b. We further suspect that the effect of increasing the number of teachers interacts with the extremity of the target knowledge concept. We believe more extreme target knowledge concepts would benefit less from increasing the number of classrooms since most students will fall on the same side of very heavily biased target knowledge concepts.

*Results.    *
As predicted, increasing the number of teachers increases the information gain with diminishing returns as the number of teachers increases. Specifically, we observe that there is a massive improvement to learning outcomes going from 1 teacher to 2 teachers especially for more moderate target knowledge concepts of 0.5, 0.6, and 0.7, and smaller and smaller improvements as additional teachers are used. As expected, having more classrooms does not particularly help teachers teach extreme target knowledge concepts, as there is no difference between few and many teachers all showing mostly Heads.

### Simulation 2b: Number of Teachers / Class Size
LINEBREAK

*Research Question.    *
Do the effects of the number of teachers by their prior personal knowledge found in Simulation 1b persist when the teachers have imperfect teacher knowledge? How do the effects compare with when the teachers have perfect teacher beliefs?

*Design and Hypotheses.    *
We use a 5 (1, 2, 3, 5, and 10) x 2 (perfect vs. imperfect teacher beliefs) experimental design. Based on our results from Simulation 1b, we hypothesized that increasing the number of teachers would strictly improve student learning rate because each classroom will be smaller, allowing teachers to pick more tailored teaching examples for their classroom. We also hypothesized that the improvement to information gain would have diminishing returns as the number of teachers increases, just as we found in Simulation 1b. We further suspect that the effect of increasing the number of teachers interacts with the extremity of the target knowledge concept. We believe more extreme target knowledge concepts would benefit less from increasing the number of classrooms since most students will fall on the same side of very heavily biased target knowledge concepts.

*Results.    *
As predicted, increasing the number of teachers increases the information gain with diminishing returns as the number of teachers increases. Specifically, we observe that there is a massive improvement to learning outcomes going from 1 teacher to 2 teachers especially for more moderate target knowledge concepts of 0.5, 0.6, and 0.7, and smaller and smaller improvements as additional teachers are used. As expected, having more classrooms does not particularly help teachers teach extreme target knowledge concepts, as there is no difference between few and many teachers all showing mostly Heads. Again, we note that this effect only exists with a sorted school policy; without sorting, increasing the number of classrooms does not guarantee a significant drop in within-classroom variance in student personal knowledge.

### Simulation 2c: Number of Assessments vs. Number of Teaching Examples
LINEBREAK

*Research Question.    *
How do the number of time steps dedicated to assessment periods vs. teaching periods affect information gain? If there exists an effect, does it interact with the number of teachers or the target knowledge concept?

*Design and Hypotheses.    *
We use a 13 (0 through 12 assessment periods) x 5 (1, 2, 3, 5, or 10 teachers) x 5 (0.5 through 0.9 target knowledge concept) design. In the 12 different time steps, teachers can either elect to conduct a formative assessment or present a teaching example (details described earlier). For the sake of simplicity, every teacher conducts the same number of assessments and shows the same number of examples, and all assessments must come before the teaching examples.

We first hypothesized that across all numbers of assessments, there will be less information gain with imperfect teacher knowledge than with perfect teacher knowledge due to errors in sorting and non-optimal teaching example selection. 

Next, we hypothesized that there would be a concave down shape in the effect that manipulating the number of assessments has on information gain. We thought that a few assessments would get the estimated beliefs about student knowledge close enough to the students' true beliefs to minimize sorting errors, leaving teachers with enough time to teach lessons to move the students' actual beliefs towards the target concept. Too many assessments would reduce the amount of time teachers have to teach regardless of how precise their estimates of student knowledge are; too few assessments would prevent teachers from selecting helpful examples in their lessons. At one extreme, with no assessments, the adminstrator and teachers would have no knowledge of the students' prior personal knowledge to base their sorting into classrooms and selection of teaching examples on, resulting in arbitrary blind guessing about which classroom a student should be grouped into and which teaching examples would be most effective. At the other extreme, if the students are assessed all 12 time steps, no matter how accurate the teacher beliefs may be, there are no opportunities to actually show teaching examples to shift the students' personal knowledge, so the students' prior and posterior personal knowledge distributions are identical and there is no improvement over the baseline.

We further hypothsized that this effect would interact with the number of teachers, where having a the optimal number of assessments for increasing information gain would be particularly high for more teachers because having more classrooms should demand more accurate sorting. Finally, we hypothesized that the effect of assessments would interact with the extremity of the target knowledge concept (i.e. the true weight of the coin), where more extreme target knowledge concepts would have a lower optimal number of assessments. We suspected that extreme targets require fewer assessments to accurately discern which classroom a student belongs in, since most students will require the same set of teaching examples to reach extreme target knowledge concepts anyway.

*Results.    *
As predicted,  we continue to find that with imperfect teacher beliefs, teachers perform strictly worse in any given regime of number of teachers, target knowledge concept, and assessment parameters. We also found the hypothesized concave down relationship between the number of assessments and information gain for target knowledge concepts between 0.5 and 0.8. We also see support for the hypothesized interaction the target knowledge concept, where higher target knowledge concepts have a lower optimal number of assessments. We did not find evidence of the hypothesized interaction between assessments and number of teachers. 

These non-linear results hint at an interesting tradeoff between assessments and teaching. When there is a moderate target knowledge concept, the average distance to the target knowledge concept is lower than when there is an extreme target knowledge concept, and students can fall on either side of the target. This favors conducting more assessments for two reasons. First, it is more important to have high accuracy in the assessment phase to discern whether students' best guess of the weight of the coin based on their prior personal knowledge falls below or above the target knowledge concept. Secondly, there is less teaching to be done because fewer teaching examples are necessary to calibrate the students' personal knowledge to be close to the target knowledge. When the target is extreme, you may need many more teaching examples in order to calibrate a student who has a very distant best guess (e.g. 0.1) to the extreme target (e.g. 0.9), and you do not need to precisely measure the students' best guess (e.g. is it 0.1 or 0.2 or 0.3?) to know that you must show many heads.

This is consistent with the seemingly aberrant results for a target knowledge concept of 0.9. With such an extreme target concept, no assessment is even necessary between 1 through 5 teachers to know that you will be showing mostly Heads regardless of the students' prior personal knowledge distributions. Only when there are 10 teachers will there be a class of students who may all consistently benefit from seeing tails because they fall at or above 0.9 in their own prior personal knowledge. As such, there is asymptotic behavior as the number of assessments increases, because the improvement over the baseline approaches zero.

```{r,include=F}
# conditions <- matrix(c("Sorted Perfect: teachers and administrators who have perfect knowledge of students and sort them into classrooms via this knowledge","Sorted Uncertain", "Unsorted Perfect", "Unsorted Uncertain" ), ncol=2)
# colnames(conditions) <- c('Perfect Teacher/Admin Knowledge', 'Uncertain Teacher/Admin Knowledge')
# rownames(conditions) <- c('Sorted', 'Unsorted')
# conditions.table <- as.table(conditions)
# kable(conditions.table)
```


```{r, include = F}
studentByStudent_df <- studentByStudent_df %>%
  mutate(simType = as.factor(simType))

studentByStudent_df_small <- studentByStudent_df %>%
#   filter(numAssessments == 6) %>%
  filter(simType=="sortedPerfectTeachers"|simType=="unsortedUncertainTeachers", numTeachers == 10, numAssessments == 0)

studentByStudent_df_small %>%
  mutate(netDKL = oldDKL - newDKL) %>%
  group_by(teacherMu, simType) %>%
  summarise(sum = sum(netDKL))

ggplot(studentByStudent_df_small,
       ##aes(x = fct_reorder(factor(studentID), oldDKL))) +
       aes(x = factor(studentID), oldDKL)) +
  #geom_point(aes(y=oldDKL)) +
  geom_point(aes(y=oldDKL-newDKL, col = simType)) +
  facet_grid(teacherMu~.)

ggplot(studentByStudent_df_small,
       aes(x = oldDKL, y=newDKL, col=simType)) +
  geom_jitter(alpha=0.1, width=.3, height = 0) +
  facet_grid(. ~ teacherMu) + 
  geom_abline(slope =1, lty =2) +
  geom_smooth(method="lm")
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
```


```{r, fig.cap="Might be good for showing how sortedPerfect and naive provide upper and lower bounds for the unsortedUncertain teachers"}
ig_by_sim <- df %>%
  group_by(simType, numTeachers, numAssessments, teacherMu) %>%
  summarise(IG = mean(IG))

ggplot(filter(ig_by_sim, teacherMu < .8),
       aes(x = numAssessments, y = IG, col = simType)) +
  geom_line() + 
  facet_grid(teacherMu ~ numTeachers) + 
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12)) + 
    ggthemes::theme_few() +
  langcog::scale_color_solarized()


```

```{r eval = FALSE, fig.cap="Probably not used"}
ggplot(filter(df_sim2),
       aes(x = numTeachers, y = mean, col = factor(teacherMu))) +
  geom_line() +
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() +
  facet_grid(interaction(sorting,knowledge)~numAssessments) +
  langcog::scale_color_solarized(name = "Target Concept") +
  theme(legend.position = "bottom") +
  ylab("Info Gain Over Naive Baseline (nats)") +
  xlab("Number of Teachers") +
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12))
```

```{r, fig.cap="Temp"}
ggplot(df_sim2,
       aes(x = numTeachers, y = mean, col = sorting, pch = knowledge, lty = knowledge)) +
  geom_line() +
  # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  ggthemes::theme_few() +
  facet_grid(teacherMu~numAssessments) +
  langcog::scale_color_solarized(name = "Target Concept") +
  theme(legend.position = "bottom") +
  ylab("Info Gain Over Naive Baseline (nats)") +
  xlab("Number of Teachers") +
  scale_x_continuous(breaks = c(0, 3, 6, 9, 12))
```

```{r, include = F}
# 
# ggplot(filter(ig_by_sim_type, teacherMu == .5, numTeachers == 5,
#               sim_type != "unsorted_uncertain"),
#        aes(x = numAssessments, y = mean, lty = sim_type)) +
#   geom_line() +
#   geom_point() +
#   ggthemes::theme_few() +
#   # langcog::scale_color_solarized(name = "Target Concept") +
#   # geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
#   # theme(legend.position = "bottom") +
#   ylab("Info Gain Over Unsorted Baseline (nats)") +
#   xlab("Number of Assessments") +
#   scale_linetype(guide=FALSE) +
#   geom_dl(aes(label = sim_type), method = "smart.grid") +
#   scale_x_continuous(breaks = c(0, 3, 6, 9, 12))
# 
# il_by_sim_type <- df_spread %>%
#   filter(numAssessments < 12, 
#          numTeachers > 2) %>%
#   mutate(IL = sortedUncertainTeachers - sortedPerfectTeachers) %>%
#   group_by(numAssessments, teacherMu) %>%
#   summarise(mean = mean(IL))
#   # multi_boot_standard(col = "IL")
# 
# ggplot(filter(il_by_sim_type, numAssessments < 12),
#        aes(x = numAssessments, y = mean, col = factor(teacherMu))) + 
#   geom_line() + 
#   # geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
#   ggthemes::theme_few() + 
#   langcog::scale_color_solarized(name = "Target Concept") +
#   theme(legend.position = "bottom") + 
#   ylab("Info Loss Relative to Perfect Knowledge (nats)") + 
#   xlab("Number of Assessments") + 
#   scale_x_continuous(breaks = c(0, 3, 6, 9, 12))
# 
# ggplot(filter(ig, teacherMu == .7), 
#        aes(x = numTeachers, y = IG, col = factor(simType))) +
#   geom_line() + 
#   facet_wrap(~ numAssessments)  + 
#   scale_colour_solarized() + 
#   ggthemes::theme_few()
```


## Simulation 4: Pareto frontier with imposed budget constraints 

<!-- - when assessment dominates, get lots of teachers. (uninteresting) -->
<!-- - when teaching cost dominates,  -->
<!--   - perfect teachers have a pareto frontier where it's still worth assessing sometimes -->
<!--   - but uncertain teachers don't usually - seems there you just hire one. -->
<!-- - tradeoff - you see frontiers in both -->

```{r}
ig_by_cost <- df %>%
  mutate(cost = numAssessments * 50 + numTeachers * numExamples * 2) %>%
  mutate(teacherMu = factor(teacherMu)) %>%
  group_by(numTeachers, numAssessments, teacherMu, simType, cost) %>%
  summarise(IG = mean(IG)) 

ggplot(filter(ig_by_cost, teacherMu == .5, 
              simType %in% c("sortedPerfectTeachers","sortedUncertainTeachers")),
       aes(x = cost, y = IG, col = factor(numTeachers))) + 
  geom_text(aes(label = numAssessments)) + 
  geom_line() + 
  facet_wrap(~simType)+
  scale_colour_solarized(name = "Number of Teachers") + 
  ggthemes::theme_few() + 
  theme(legend.position = "bottom") + 
  xlab("Cost") + 
  ylab("Information Gain (nats)")
```

We hypothesized that for a particular budget constraint and fixed costs of teachers and assessment, there will be a Pareto frontier of possible allocations of that budget towards teachers and assessments on which there will be an optimal allocation that maximizes student learning. Furthermore, we hypothesized that the individual results about number of teachers number of assessments would hold within levels of the other. This is the unification of all previous simulations into one model that allows us to introduce a real-world constraint of budget.

In this simulation, we test all integer combinations of number of teachers between 1 and 10 with number of assessments between 0 and 5. We assigned the cost of each teacher to be CT = \$10 and the cost of each assessment CA = \$20.

Our baselines were the worst-performing within-target setup. This always happened to be the 1-teacher 0-assessment setup, which is consistent with existing education literature, since a single teacher cannot significantly differentiate learning, and still has a lot of uncertainty about student beliefs because no assessments are performed.

Our simulated findings support our hypothesis. There is con- siderable support for an interaction model between teachers, assessments, and the target bias, F(7, 112) = 48.61, p<.001. To visualize the pareto frontier, we draw a heatmap of scores

above baseline along teacher and assessment axes. We con- tinue to see that the effects continue to be weaker for more extreme learning concepts, t(294) = -11.09, p<.001. This result can be seen in Figure 5.

Since real-world learning concepts are essentially non- negotiable, the effectiveness of a particular allocation of bud- get should be judged within-target, i.e. an optimal setup within the target bias = 0.8 level should not be penalized for being ineffective in comparison to the optimal setup for teaching a target bias of 0.5. We normalized the information gain above baseline within each target bias level. The results can be seen in 6

Consistent with Simulation 3, increasing the number of assess- ments on average improves learning rate up until you cannot
afford any more assessments, across all levels of teachers and target biases. Consistent with Simulation 2, increasing the number of assessments improves learning rate up until you cannot afford any more assessments, across all levels of assessments and target biases of 0.5, 0.6, and 0.7.

An interesting result, however, is that having noisy students diminishes the effectiveness of increasing the number of teach- ers at certain a target bias of 0.8. While we aren't sure of the specific mechanism by which this happens, we suspect that having more teachers introduces additional risk of wrongly sorting students' prior beliefs, which would increase the het- erogeneity of classrooms. Since a target belief of 0.8 is a possible target belief to show precisely with 5 examples (the basis of our simulations) but also extreme enough that most
students's prior belief will fall on one side of that target, in- creasing the number of teachers might actually reduce the effectiveness of the teacher's choice of examples.
A novel finding that emerged not described in existing educa- tion literature is that there are different patterns of optimality along the pareto frontiers. As the extremity of the target bias moves from 0.5 to 0.7, we see that the IG-optimal allocation of budget shifts from about 5-6 teachers and 2 assessments towards more assessments at the expense of the number of teachers.

# Discussion

Summary

In our simulations, we refer to the entire group of students as a "school," broken into "classrooms," and students are given "assessments" and "lessons." These mappings are inexact, however, and the analogical correspondence can be made at a lower level. For example, the same set of ideas could be applied *within a classroom*, for example to the decision whether to split the class into smaller groups and divide the teachers time amongst supervising them. And similarly, what we describe "assessments" and "lessons" could be either multi-item tests and then several days of instruction or -- again at a more micro scale -- a single question posed to the class followed by several pieces of related content within a single class session. Our goal is to elucidate general principles that govern the process of teaching rather than to claim a correspondence with specific phenomena or teaching methodology. 

Overall, we were able to replicate many real-world findings from education literature. We found that sorting students into ability level classrooms, increasing assessment, and decreasing class size all increase information gain.

One of the more interesting findings was the influence of the target bias on the effects we were finding. Based on our results, target bias significantly moderates each of the effects we found. When teaching an extremely difficult (or easy, given symmetry) learning concept such that all student prior beliefs are on one side of the concept, the payoffs to information gain that sorting, class sizes, and assessment have diminish because regardless of the level of noise, teachers have relative certainty about the students' relative level of understanding. Similarly, target biases that are very close to central (around 0.5) have a lot to gain from diminishing noise in the system.

Finally, we found that the relative effectiveness of number of teachers and and the number of assessments varies depend- ing on the target learning concept. We saw that the most optimal distribution of budget on the pareto frontier shifted towards reducing student noise when the target bias was more extreme--this seemed consistent with intuition, since having more teachers doesn't allow them to better select examples if the administrator produces greater sorting errors, resulting in heterogeneous classrooms where each teacher has less cer- tainty about whether their examples will be useful for any particular student or not.

While a lot of aspects of school learning (peer effects, social dynamics, etc.) are not built into our model and would be difficult to capture algorithmically, we believe that there is a lot to be gained from simulating classroom dynamics. Using stochastic modeling is particularly useful when exploring the effectiveness of education policy measures for a handful of reasons. Fundamentally, the merits of stochastic modeling arise from the use of synthetic data. Since the studies are computer-simulated, there are no human subjects in the stud- ies, and all of the data is synthetic. This allows researchers to take greater liberties in experimental design because there are no ethical concerns. For instance, we can execute a sim- ulation that will intentionally misclassify synthetic students into classrooms that are not appropriate for their achievement level to understand the negative impact doing so has on both the misclassified student and his peers. Such a study would not be ethically permissible in an actual school.

Furthermore, no two schools are the same, and the use of mod- eling and synthetic data enables rapid iteration over different simulated schools with different simulated students. We can quickly test whether the purported benefit to student learning of various education interventions holds with, for example, a limited budget, or highly varied student achievement levels, or immensely large class sizes. There is also no risk of student subject attrition that may jeopardize real-world school studies; synthetic students do not exhibit unpredictable absenteeism, transfer schools, take sick days, etc. As such, we can cus- tomize our interventions based on school parameters, validate the robustness of our results, and better infer generalized find- ings about education policy without incurring astronomical experimental monetary costs nor require lengthy longitudinal study. With a complete stochastic model, we can quickly scale the number of hypotheses we test and the variety of schools we test them on.
Our series of simulations are designed to replicate real-world results from education literature in a quantitative, simulated fashion. The first few simulations looked at a lot of existing educational theory in isolation, while our final simulation attempts to unify these different design aspects of a school system into a more holistic view. We found that increasing the number of teachers (and thus decreasing class size) and increasing the amount of assessment to improve precision about student beliefs increases student learning, as predicted in real-world school studies.

Importantly, we were able to generate pareto frontiers allocating a fixed shared resource of budget into the dimensions of teachers and sorting. The ability to make sense of the tradeoffs that school administrators and policymakers need to
make when designing their schools can help improve education. This would be particularly impactful in communities of low socioeconomic status where academic achievement tends to be lower because budgets are particularly constrained and attracting teaching staff is difficult.


Some of the limitations of the present work is in the assump- tions that we make to build our model. Most obvious is the simplicity of the learning task we described during the intro- duction. Most real-world learning tasks are more complicated and may have multiple objectives. Furthermore learning in the real-world is affected by a lot of more abstract facets: peer learning and social environment, parenting styles, stereotype threat, among other things. These are not captured in the model, and would be difficult to capture in any model.
The agnostic prior beliefs we 1) use for teachers to generate examples from; 2) generate student prior beliefs from; and 3) create target bias distributions with are all uniform distri- butions. These are not necessarily the case. Respectively, 1) teachers may have prior beliefs about which examples are more effective, akin to pedagogical knowledge or pedagogical content knowledge (Cochran, 1991); 2) students may tend towards certain common prior beliefs/misconceptions about a particular learning concept; 3) more extreme learning concepts may be more rare, and thus the admin should weight more heavily the pareto frontier of moderate target bias values.


# Acknowledgements

This work supported by NSF BCS #1456077. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent


